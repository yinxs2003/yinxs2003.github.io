<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Flink Connect和Union区别</title>
      <link href="/2020/11/03/Flink%20Connect%E5%92%8CUnion%E5%8C%BA%E5%88%AB/"/>
      <url>/2020/11/03/Flink%20Connect%E5%92%8CUnion%E5%8C%BA%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<p><code>Flink DataStream</code>中<code>union</code>和<code>connect</code>都有一个共同的作用，就是将2个流或多个流合成一个流。但是两者的区别是：<code>union</code>连接的2个流的类型必须一致，<code>connect</code>连接的流可以不一致，但是可以统一处理。</p><p>具体看下面示例：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ConnectOperator</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;        StreamExecutionEnvironment sEnv = StreamExecutionEnvironment.getExecutionEnvironment();        sEnv.setParallelism(<span class="hljs-number">1</span>);        Properties p = <span class="hljs-keyword">new</span> Properties();        p.setProperty(<span class="hljs-string">"bootstrap.servers"</span>, <span class="hljs-string">"localhost:9092"</span>);        SingleOutputStreamOperator&lt;Student&gt; student = sEnv                .addSource(<span class="hljs-keyword">new</span> FlinkKafkaConsumer010&lt;String&gt;(<span class="hljs-string">"student"</span>, <span class="hljs-keyword">new</span> SimpleStringSchema(), p))                .map(<span class="hljs-keyword">new</span> MapFunction&lt;String, Student&gt;() &#123;                    <span class="hljs-meta">@Override</span>                    <span class="hljs-function"><span class="hljs-keyword">public</span> Student <span class="hljs-title">map</span><span class="hljs-params">(String value)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;                        <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> Gson().fromJson(value, Student<span class="hljs-class">.<span class="hljs-keyword">class</span>)</span>;                    &#125;                &#125;);        student.print();        SingleOutputStreamOperator&lt;Teacher&gt; teacher = sEnv                .addSource(<span class="hljs-keyword">new</span> FlinkKafkaConsumer010&lt;String&gt;(<span class="hljs-string">"teacher"</span>, <span class="hljs-keyword">new</span> SimpleStringSchema(), p))                .map(<span class="hljs-keyword">new</span> MapFunction&lt;String, Teacher&gt;() &#123;                    <span class="hljs-meta">@Override</span>                    <span class="hljs-function"><span class="hljs-keyword">public</span> Teacher <span class="hljs-title">map</span><span class="hljs-params">(String value)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;                        <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> Gson().fromJson(value, Teacher<span class="hljs-class">.<span class="hljs-keyword">class</span>)</span>;                    &#125;                &#125;);        teacher.print();        ConnectedStreams&lt;Student, Teacher&gt; connect = student.connect(teacher);        connect.process(<span class="hljs-keyword">new</span> CoProcessFunction&lt;Student, Teacher, Tuple5&lt;String, Integer, String, String, Long&gt;&gt;() &#123;            <span class="hljs-meta">@Override</span>            <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">processElement1</span><span class="hljs-params">(Student value, Context ctx, Collector&lt;Tuple5&lt;String, Integer, String, String, Long&gt;&gt; out)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;                out.collect(<span class="hljs-keyword">new</span> Tuple5&lt;&gt;(value.name, value.age, value.sex, value.classId, value.timestamp));            &#125;            <span class="hljs-meta">@Override</span>            <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">processElement2</span><span class="hljs-params">(Teacher value, Context ctx, Collector&lt;Tuple5&lt;String, Integer, String, String, Long&gt;&gt; out)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;                out.collect(<span class="hljs-keyword">new</span> Tuple5&lt;&gt;(value.name, value.age, value.sex, value.classId, value.timestamp));            &#125;        &#125;).print(<span class="hljs-string">"process"</span>);        <span class="hljs-comment">// connect</span>        connect.map(<span class="hljs-keyword">new</span> CoMapFunction&lt;Student, Teacher, Tuple5&lt;String, Integer, String, String, Long&gt;&gt;() &#123;            <span class="hljs-meta">@Override</span>            <span class="hljs-function"><span class="hljs-keyword">public</span> Tuple5&lt;String, Integer, String, String, Long&gt; <span class="hljs-title">map1</span><span class="hljs-params">(Student value)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;                <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> Tuple5&lt;&gt;(value.name, value.age, value.sex, value.classId, value.timestamp);            &#125;            <span class="hljs-meta">@Override</span>            <span class="hljs-function"><span class="hljs-keyword">public</span> Tuple5&lt;String, Integer, String, String, Long&gt; <span class="hljs-title">map2</span><span class="hljs-params">(Teacher value)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;                <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> Tuple5&lt;&gt;(value.name, value.age, value.sex, value.classId, value.timestamp);            &#125;        &#125;).print(<span class="hljs-string">"map"</span>);        <span class="hljs-comment">// union</span>        student.map(<span class="hljs-keyword">new</span> MapFunction&lt;Student, Tuple5&lt;String, Integer, String, String, Long&gt;&gt;() &#123;            <span class="hljs-meta">@Override</span>            <span class="hljs-function"><span class="hljs-keyword">public</span> Tuple5&lt;String, Integer, String, String, Long&gt; <span class="hljs-title">map</span><span class="hljs-params">(Student value)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;                <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> Tuple5&lt;&gt;(value.name, value.age, value.sex, value.classId, value.timestamp);            &#125;        &#125;).union(teacher.map(<span class="hljs-keyword">new</span> MapFunction&lt;Teacher, Tuple5&lt;String, Integer, String, String, Long&gt;&gt;() &#123;            <span class="hljs-meta">@Override</span>            <span class="hljs-function"><span class="hljs-keyword">public</span> Tuple5&lt;String, Integer, String, String, Long&gt; <span class="hljs-title">map</span><span class="hljs-params">(Teacher value)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;                <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> Tuple5&lt;&gt;(value.name, value.age, value.sex, value.classId, value.timestamp);            &#125;        &#125;)).print(<span class="hljs-string">"union"</span>);        sEnv.execute(<span class="hljs-string">"ConnectOperator"</span>);    &#125;&#125;</code></pre><p>connect可以将2个不同类型的流同时用不同的逻辑处理好，形成一个流。</p><p>union是将2个同类型的流，合成一个，进行处理。</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java8的Function和BI_Function</title>
      <link href="/2020/10/20/Java8%E7%9A%84Function%E5%92%8CBI_Function/"/>
      <url>/2020/10/20/Java8%E7%9A%84Function%E5%92%8CBI_Function/</url>
      
        <content type="html"><![CDATA[<h1 id="Function函数"><a href="#Function函数" class="headerlink" title="Function函数"></a>Function函数</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p> Function作为一个函数式接口，主要方法apply接受一个参数，返回一个值</p><h2 id="举个例子"><a href="#举个例子" class="headerlink" title="举个例子"></a>举个例子</h2><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.bd.thread;<span class="hljs-keyword">import</span> lombok.extern.slf4j.Slf4j;<span class="hljs-keyword">import</span> java.util.function.Function;<span class="hljs-meta">@Slf</span>4j<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Test</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        Function&lt;Integer, Integer&gt; f1 = x -&gt; &#123;            <span class="hljs-keyword">return</span> x * x;        &#125;;        Function&lt;Integer, Integer&gt; f2 = x -&gt; &#123;            <span class="hljs-keyword">return</span> x + x;        &#125;;                      <span class="hljs-comment">// 表示f1参数由f2函数构成，即(x + x) * (x + x)</span>        <span class="hljs-comment">//（3+3）*（3+3）= 36</span>        log.info(<span class="hljs-string">"&#123;&#125;"</span>, f1.compose(f2).apply(<span class="hljs-number">3</span>));                <span class="hljs-comment">// 表示先执行f1函数，再执行f2函数</span>        <span class="hljs-comment">//（3*3）+（3*3）=18</span>        log.info(<span class="hljs-string">"&#123;&#125;"</span>, f1.andThen(f2).apply(<span class="hljs-number">3</span>));    &#125;&#125;</code></pre><h2 id="Function接口源码如下"><a href="#Function接口源码如下" class="headerlink" title="Function接口源码如下"></a>Function接口源码如下</h2><pre><code class="hljs java"><span class="hljs-meta">@FunctionalInterface</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">interface</span> <span class="hljs-title">Function</span>&lt;<span class="hljs-title">T</span>, <span class="hljs-title">R</span>&gt; </span>&#123;    <span class="hljs-function">R <span class="hljs-title">apply</span><span class="hljs-params">(T t)</span></span>;       <span class="hljs-comment">// 表示f1参数由f2函数构成，即(x + x) * (x + x)</span>    <span class="hljs-keyword">default</span> &lt;V&gt; <span class="hljs-function">Function&lt;V, R&gt; <span class="hljs-title">compose</span><span class="hljs-params">(Function&lt;? <span class="hljs-keyword">super</span> V, ? extends T&gt; before)</span> </span>&#123;        Objects.requireNonNull(before);        <span class="hljs-keyword">return</span> (V v) -&gt; apply(before.apply(v));    &#125;    <span class="hljs-comment">// 表示先执行f1函数，再执行f2函数</span>    <span class="hljs-keyword">default</span> &lt;V&gt; <span class="hljs-function">Function&lt;T, V&gt; <span class="hljs-title">andThen</span><span class="hljs-params">(Function&lt;? <span class="hljs-keyword">super</span> R, ? extends V&gt; after)</span> </span>&#123;        Objects.requireNonNull(after);        <span class="hljs-keyword">return</span> (T t) -&gt; after.apply(apply(t));    &#125;&#125;</code></pre><h1 id="BIFunction函数"><a href="#BIFunction函数" class="headerlink" title="BIFunction函数"></a>BIFunction函数</h1><p>BiFunction也是一个函数式接口，和Function接口不同的是，它在接口中声明了3个泛型，其中前两个作为方法参数类型，最后一个作为返回类型</p><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.bd.thread;<span class="hljs-keyword">import</span> lombok.extern.slf4j.Slf4j;<span class="hljs-keyword">import</span> java.util.function.BiFunction;<span class="hljs-keyword">import</span> java.util.function.Function;<span class="hljs-meta">@Slf</span>4j<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Test</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        BiFunction&lt;String, String, String&gt; f1 = (x, y) -&gt; <span class="hljs-string">"world "</span> + x + y;        Function&lt;String, String&gt; f2 = (x) -&gt; <span class="hljs-string">"hello "</span> + x;        <span class="hljs-comment">// world lisi</span>        log.info(f1.apply(<span class="hljs-string">"li"</span>, <span class="hljs-string">"si"</span>));                <span class="hljs-comment">// hello + world lisi</span>        log.info(f1.andThen(f2).apply(<span class="hljs-string">"li"</span>, <span class="hljs-string">"si"</span>));    &#125;&#125;</code></pre><p>根据这个例子可以看出</p><p>f1.andThen(f2)是<strong>先计算f1且将结果作为参数传入f2中</strong></p><p>BiFunction接口源码如下，可以看出由after（f2参数）调用apply方法执行f1的apply方法，即将f1计算结果传入f2中</p><pre><code class="hljs java"><span class="hljs-meta">@FunctionalInterface</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">interface</span> <span class="hljs-title">BiFunction</span>&lt;<span class="hljs-title">T</span>, <span class="hljs-title">U</span>, <span class="hljs-title">R</span>&gt; </span>&#123;    <span class="hljs-function">R <span class="hljs-title">apply</span><span class="hljs-params">(T t, U u)</span></span>;    <span class="hljs-keyword">default</span> &lt;V&gt; <span class="hljs-function">BiFunction&lt;T, U, V&gt; <span class="hljs-title">andThen</span><span class="hljs-params">(Function&lt;? <span class="hljs-keyword">super</span> R, ? extends V&gt; after)</span> </span>&#123;        Objects.requireNonNull(after);        <span class="hljs-keyword">return</span> (T t, U u) -&gt; after.apply(apply(t, u));    &#125;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java中如何自动调用父类中的资源初始化方法</title>
      <link href="/2020/10/20/Java%E4%B8%AD%E5%A6%82%E4%BD%95%E8%87%AA%E5%8A%A8%E8%B0%83%E7%94%A8%E7%88%B6%E7%B1%BB%E4%B8%AD%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95/"/>
      <url>/2020/10/20/Java%E4%B8%AD%E5%A6%82%E4%BD%95%E8%87%AA%E5%8A%A8%E8%B0%83%E7%94%A8%E7%88%B6%E7%B1%BB%E4%B8%AD%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1 id="一个简单的例子"><a href="#一个简单的例子" class="headerlink" title="一个简单的例子"></a>一个简单的例子</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在写程序时候常常有一些资源初始化方法，我们希望这些方法能够被自动调用 ，可以使用如下方式实现自动调用</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>父类代码如下：</p><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.bd.autocall;<span class="hljs-keyword">import</span> lombok.extern.slf4j.Slf4j;<span class="hljs-meta">@Slf</span>4j<span class="hljs-keyword">public</span> <span class="hljs-keyword">abstract</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Super</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">abstract</span> <span class="hljs-keyword">void</span> <span class="hljs-title">call0</span><span class="hljs-params">()</span></span>;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">call</span><span class="hljs-params">()</span></span>&#123;        <span class="hljs-comment">// 在这里调用子类中的call0方法</span>        <span class="hljs-keyword">this</span>.call0();    &#125;&#125;</code></pre><p>子类代码如下：</p><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.bd.autocall;<span class="hljs-keyword">import</span> lombok.extern.slf4j.Slf4j;<span class="hljs-meta">@Slf</span>4j<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Sub</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Super</span> </span>&#123;    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">call0</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-comment">// 这里写被初始化的方法</span>        log.info(<span class="hljs-string">"son call0"</span>);    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        Sub a = <span class="hljs-keyword">new</span> Sub();        a.call();    &#125;&#125;</code></pre><p>执行结果如下：</p><pre><code class="hljs basic"><span class="hljs-symbol">1 </span>[main] INFO <span class="hljs-keyword">com</span>.bd.autocall.Sub - son call0</code></pre><p>其中，call方法是对外方法，会被其他人调用</p><p>其他人调用父类的call方法的时，子类的call0方法也会被调用，从而实现call0方法被自动调用</p><h1 id="复杂例子"><a href="#复杂例子" class="headerlink" title="复杂例子"></a>复杂例子</h1><h2 id="背景-1"><a href="#背景-1" class="headerlink" title="背景"></a>背景</h2><p>在Flink中，自定义一个RichFunction，其中open函数里是初始化缓存或者数据资源的方法，该方法会被Flink框架自动调用且只调用一次。我们希望在实现子类时候，即能继承open函数原有的资源，又能向open函数中添加新的资源初始化</p><h2 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h2><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">abstract</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">AbstractRichFunction</span> <span class="hljs-keyword">implements</span> <span class="hljs-title">RichFunction</span>, <span class="hljs-title">AthenaCodisInterface</span>, <span class="hljs-title">AthenaHbaseInterface</span>, <span class="hljs-title">AthenaMediaJdbcInterface</span>, <span class="hljs-title">Serializable</span> </span>&#123;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> Logger logger = LoggerFactory.getLogger(AbstractRichFunction<span class="hljs-class">.<span class="hljs-keyword">class</span>)</span>;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-keyword">long</span> serialVersionUID = <span class="hljs-number">1L</span>;    <span class="hljs-keyword">private</span> External external;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">transient</span> CodisExternalClient&lt;String, MediaMessage&gt; mediaCodisClient;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">transient</span> CodisExternalClient&lt;String, PostMessage&gt; postCodisClient;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">transient</span> CodisExternalClient&lt;String, Long&gt; shunCodisClient;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">transient</span> HbaseExternalClient&lt;String, PostMessage&gt; postHbaseClient;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">transient</span> JdbcExternalClient&lt;String, MediaPostEventMessage&gt; weMediaJdbcClient;    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">open</span><span class="hljs-params">(Configuration parameters)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;        ParameterTool parameterTool = (ParameterTool)getRuntimeContext().getExecutionConfig().getGlobalJobParameters();        String athenaConfig = parameterTool.get(ATHENA_CONFIG_NAME);        <span class="hljs-keyword">this</span>.external = JSONObject.parseObject(athenaConfig, External<span class="hljs-class">.<span class="hljs-keyword">class</span>)</span>;        <span class="hljs-keyword">this</span>.open0(parameters);    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">abstract</span> <span class="hljs-keyword">void</span> <span class="hljs-title">open0</span><span class="hljs-params">(Configuration parameters)</span> <span class="hljs-keyword">throws</span> Exception</span>;      <span class="hljs-comment">// --------------------------------------------------------------------------------------------</span>    <span class="hljs-comment">//  Runtime context access</span>    <span class="hljs-comment">// --------------------------------------------------------------------------------------------</span>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">transient</span> RuntimeContext runtimeContext;    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">setRuntimeContext</span><span class="hljs-params">(RuntimeContext t)</span> </span>&#123;        <span class="hljs-keyword">this</span>.runtimeContext = t;    &#125;    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> RuntimeContext <span class="hljs-title">getRuntimeContext</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-keyword">if</span> (<span class="hljs-keyword">this</span>.runtimeContext != <span class="hljs-keyword">null</span>) &#123;            <span class="hljs-keyword">return</span> <span class="hljs-keyword">this</span>.runtimeContext;        &#125; <span class="hljs-keyword">else</span> &#123;            <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> IllegalStateException(<span class="hljs-string">"The runtime context has not been initialized."</span>);        &#125;    &#125;    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> IterationRuntimeContext <span class="hljs-title">getIterationRuntimeContext</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-keyword">if</span> (<span class="hljs-keyword">this</span>.runtimeContext == <span class="hljs-keyword">null</span>) &#123;            <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> IllegalStateException(<span class="hljs-string">"The runtime context has not been initialized."</span>);        &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (<span class="hljs-keyword">this</span>.runtimeContext <span class="hljs-keyword">instanceof</span> IterationRuntimeContext) &#123;            <span class="hljs-keyword">return</span> (IterationRuntimeContext) <span class="hljs-keyword">this</span>.runtimeContext;        &#125; <span class="hljs-keyword">else</span> &#123;            <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> IllegalStateException(<span class="hljs-string">"This stub is not part of an iteration step function."</span>);        &#125;    &#125;    <span class="hljs-comment">// --------------------------------------------------------------------------------------------</span>    <span class="hljs-comment">//  Default life cycle methods</span>    <span class="hljs-comment">// --------------------------------------------------------------------------------------------</span>    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">close</span><span class="hljs-params">()</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;&#125;    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">synchronized</span> CodisExternalClient&lt;String, MediaMessage&gt; <span class="hljs-title">getMediaCodisClient</span><span class="hljs-params">()</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;        <span class="hljs-keyword">if</span>(mediaCodisClient != <span class="hljs-keyword">null</span>) <span class="hljs-keyword">return</span> mediaCodisClient;        Map&lt;AthenaExternalEnum, CodisExternal&gt; map = external.getCodis();        CodisExternal codisExternal = map.get(AthenaExternalEnum.MEDIA_CACHE_EVENT);        mediaCodisClient = codisExternal.getClient();        <span class="hljs-keyword">return</span> mediaCodisClient;    &#125;    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">synchronized</span> CodisExternalClient&lt;String, PostMessage&gt; <span class="hljs-title">getPostCodisClient</span><span class="hljs-params">()</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;        <span class="hljs-keyword">if</span>(postCodisClient != <span class="hljs-keyword">null</span>) <span class="hljs-keyword">return</span> postCodisClient;        Map&lt;AthenaExternalEnum, CodisExternal&gt; map = external.getCodis();        CodisExternal codisExternal = map.get(AthenaExternalEnum.POST_CACHE_EVENT);        postCodisClient = codisExternal.getClient();        <span class="hljs-keyword">return</span> postCodisClient;    &#125;    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">synchronized</span> CodisExternalClient&lt;String, Long&gt; <span class="hljs-title">getShunCodisClient</span><span class="hljs-params">()</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;        <span class="hljs-keyword">if</span>(shunCodisClient != <span class="hljs-keyword">null</span>) <span class="hljs-keyword">return</span> shunCodisClient;        Map&lt;AthenaExternalEnum, CodisExternal&gt; map = external.getCodis();        CodisExternal codisExternal = map.get(AthenaExternalEnum.SHUN_CACHE_EVENT);        shunCodisClient = codisExternal.getClient();        <span class="hljs-keyword">return</span> shunCodisClient;    &#125;    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">synchronized</span> JdbcExternalClient&lt;String, MediaPostEventMessage&gt; <span class="hljs-title">getWeMediaJdbcClient</span><span class="hljs-params">()</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;        <span class="hljs-keyword">if</span>(weMediaJdbcClient != <span class="hljs-keyword">null</span>) <span class="hljs-keyword">return</span> weMediaJdbcClient;        Map&lt;AthenaExternalEnum, JdbcExternal&gt; map = external.getJdbc();        JdbcExternal jdbcExternal = map.get(AthenaExternalEnum.WEMEDIA_MYSQL_EVENT);        <span class="hljs-keyword">return</span> jdbcExternal.getClient();    &#125;    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">synchronized</span> HbaseExternalClient&lt;String, PostMessage&gt; <span class="hljs-title">getPostHbaseClient</span><span class="hljs-params">()</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;        <span class="hljs-keyword">return</span> external.getHbase().get(AthenaExternalEnum.POST_HBASE_EVENT).getClient();    &#125;    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> HbaseExternalClient&lt;String, MediaMessage&gt; <span class="hljs-title">getMediaHbaseClient</span><span class="hljs-params">()</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;        <span class="hljs-keyword">return</span> external.getHbase().get(AthenaExternalEnum.MEDIA_HBASE_EVENT).getClient();    &#125;&#125;</code></pre><p>子类</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">abstract</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">RichFlatMapFunction</span>&lt;<span class="hljs-title">IN</span>, <span class="hljs-title">OUT</span>&gt; <span class="hljs-keyword">extends</span> <span class="hljs-title">AbstractRichFunction</span> <span class="hljs-keyword">implements</span> <span class="hljs-title">FlatMapFunction</span>&lt;<span class="hljs-title">IN</span>, <span class="hljs-title">OUT</span>&gt; </span>&#123;    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">abstract</span> <span class="hljs-keyword">void</span> <span class="hljs-title">flatMap</span><span class="hljs-params">(IN value, Collector&lt;OUT&gt; out)</span> <span class="hljs-keyword">throws</span> Exception</span>;&#125;</code></pre><p>子类2</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PostMapFunction</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">RichFlatMapFunction</span>&lt;<span class="hljs-title">PostFactEventMessage</span>, <span class="hljs-title">PostFactEventMessage</span>&gt; </span>&#123;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> Logger LOG = LoggerFactory.getLogger(PostFactMsgMapFunction<span class="hljs-class">.<span class="hljs-keyword">class</span>)</span>;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">transient</span> HbaseExternalClient&lt;String, PostMessage&gt; postHbaseClient;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">transient</span> LoadingCache&lt;String, PostMessage&gt; loadingCache;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">transient</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> AtomicLong codisCount = <span class="hljs-keyword">new</span> AtomicLong(<span class="hljs-number">0</span>);    <span class="hljs-keyword">private</span> <span class="hljs-keyword">transient</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> AtomicLong hbaseCount = <span class="hljs-keyword">new</span> AtomicLong(<span class="hljs-number">0</span>);    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">open0</span><span class="hljs-params">(Configuration parameters)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;        postHbaseClient = getPostHbaseClient();        loadingCache = CacheBuilder.newBuilder()                .maximumSize(<span class="hljs-number">5000000</span>)                .expireAfterWrite(<span class="hljs-number">5</span>, TimeUnit.MINUTES)                .build(<span class="hljs-keyword">new</span> CacheLoader&lt;String, PostMessage&gt;() &#123;                    <span class="hljs-meta">@Override</span>                    <span class="hljs-function"><span class="hljs-keyword">public</span> PostMessage <span class="hljs-title">load</span><span class="hljs-params">(String docId)</span> <span class="hljs-keyword">throws</span> IOException </span>&#123;                        PostMessage postMessage = <span class="hljs-keyword">null</span>;                        <span class="hljs-keyword">try</span> &#123;                            postMessage = getPostCodisClient().getObject(docId);                        &#125; <span class="hljs-keyword">catch</span> (Exception e) &#123;                            LOG.warn(<span class="hljs-string">"codis error"</span>, e);                        &#125;                        LOG.error(<span class="hljs-string">"load postMessage from codis count=&#123;&#125;"</span>, codisCount.addAndGet(<span class="hljs-number">1</span>));                        <span class="hljs-keyword">if</span> (Objects.isNull(postMessage) &amp;&amp; StringUtils.isNotBlank(docId)) &#123;                            postMessage = postHbaseClient.getObject(docId);                            LOG.error(<span class="hljs-string">"load postMessage from hbase count=&#123;&#125;"</span>, hbaseCount.addAndGet(<span class="hljs-number">1</span>));                            LOG.error(<span class="hljs-string">"load postMessage from hbase finish"</span>);                        &#125;                        <span class="hljs-keyword">if</span> (Objects.isNull(postMessage)) &#123;                            postMessage = <span class="hljs-keyword">new</span> PostMessage();                            postMessage.setNewsId(docId);                        &#125;                        <span class="hljs-keyword">return</span> postMessage;                    &#125;                &#125;);    &#125;    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">flatMap</span><span class="hljs-params">(PostFactEventMessage value, Collector&lt;PostFactEventMessage&gt; out)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;        PostMessage postMessage = loadingCache.get(value.getDocId());        <span class="hljs-keyword">if</span> (Objects.isNull(postMessage)) &#123;            LOG.error(<span class="hljs-string">"postMessage=null, docId = &#123;&#125;"</span>, value.getDocId());            <span class="hljs-keyword">return</span>;        &#125;        <span class="hljs-keyword">if</span>(postMessage.getMediaId() == <span class="hljs-keyword">null</span>) <span class="hljs-keyword">return</span>;        value.loadPostMessage(postMessage);        out.collect(value);    &#125;&#125;</code></pre><p>从代码中可以看出，PostMapFunction类在open0中初始化了postHbaseClient资源，并包装成guava缓存读取器。由于open0被父类调用，postHbaseClient会被加载到open函数里，并被flink作为分布式资源使用</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Centos7安装python3</title>
      <link href="/2020/07/20/Centos7%E5%AE%89%E8%A3%85python3/"/>
      <url>/2020/07/20/Centos7%E5%AE%89%E8%A3%85python3/</url>
      
        <content type="html"><![CDATA[<h2 id="下载python3安装包"><a href="#下载python3安装包" class="headerlink" title="下载python3安装包"></a>下载python3安装包</h2><p>wget -c <a href="https://www.python.org/ftp/python/3.7.3/Python-3.7.3.tgz" target="_blank" rel="noopener">https://www.python.org/ftp/python/3.7.3/Python-3.7.3.tgz</a></p><h2 id="安装linux依赖"><a href="#安装linux依赖" class="headerlink" title="安装linux依赖"></a>安装linux依赖</h2><p>yum install libffi-devel openssl openssl-devel sqlite-devel bzip2-devel -y</p><p>在源码安装文件中编辑文件Modules/Setup.dist</p><h2 id="安装openssl"><a href="#安装openssl" class="headerlink" title="安装openssl"></a>安装openssl</h2><p>在源码安装文件中编辑文件Modules/Setup.dist</p><p>去掉如下注释</p><pre><code class="hljs python"><span class="hljs-comment">#SSL=/usr/local/ssl</span>_ssl _ssl.c \ -DUSE_SSL -I$(SSL)/include -I$(SSL)/include/openssl \ -L$(SSL)/lib -lssl -lcrypto</code></pre><h2 id="编译安装"><a href="#编译安装" class="headerlink" title="编译安装"></a>编译安装</h2><p>mkdir /usr/python3</p><p>cd Python-3.7.3</p><p>./configure –prefix=/usr/python3 –enable-loadable-sqlite-extensions &amp;&amp; make &amp;&amp; make install</p><p>ln -s /usr/python3/bin/python3 /usr/bin/python3</p><p>ln -s /usr/python3/bin/pip3 /usr/bin/pip3</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何搭建个人仓库</title>
      <link href="/2020/07/11/%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E4%BB%93%E5%BA%93/"/>
      <url>/2020/07/11/%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E4%BB%93%E5%BA%93/</url>
      
        <content type="html"><![CDATA[<h1 id="ssh端口为22情况"><a href="#ssh端口为22情况" class="headerlink" title="ssh端口为22情况"></a>ssh端口为22情况</h1><p>在服务器端172.27.9.121搭建仓库</p><p>1.在仓库目录/tmp/myproject.git输入git init</p><p>2.允许上传代码</p><pre><code class="hljs shell">git config receive.denyCurrentBranch ignore</code></pre><a id="more"></a><p>3.在本地clone远程仓库代码</p><pre><code class="hljs shell">git clone root@172.27.9.121:/tmp/myproject.git</code></pre><p>4.向121服务器~/.ssh/authxxx 添加私钥，免密码认证</p><h1 id="如何上传非ssh端口代码"><a href="#如何上传非ssh端口代码" class="headerlink" title="如何上传非ssh端口代码"></a>如何上传非ssh端口代码</h1><p>编辑本地文件~/.ssh/config，增加如下内容,remotevps代表的是远端服务器</p><pre><code class="hljs routeros"><span class="hljs-comment"># For all hosts</span>ServerAliveInterval 10Host remotevpsHostName 107.123.123.123Port 12345User rootHost *  UseKeychain <span class="hljs-literal">yes</span>  AddKeysToAgent <span class="hljs-literal">yes</span>  IdentityFile ~/.ssh/id_rsaHost *    IdentitiesOnly <span class="hljs-literal">yes</span></code></pre><p>设置远程push链接</p><pre><code class="hljs shell">git remote add origin root@remotevps:/opt/git/resumegit remote add origin</code></pre><p>克隆远端代码仓库命令</p><pre><code class="hljs shell">git clone root@remotevps:/home/yKF6600/OasisSpark2</code></pre>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>面试-Kafka专题</title>
      <link href="/2020/07/11/%E9%9D%A2%E8%AF%95-kafka%E4%B8%93%E9%A2%98/"/>
      <url>/2020/07/11/%E9%9D%A2%E8%AF%95-kafka%E4%B8%93%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h1 id="使用Kafka时会遇到的问题"><a href="#使用Kafka时会遇到的问题" class="headerlink" title="使用Kafka时会遇到的问题"></a>使用Kafka时会遇到的问题</h1><ol><li>Kafka速度很快的原因</li><li>如何保证消息不丢失</li><li>Consumer重复消费怎么处理</li><li>如何保证消息的有序性</li></ol><a id="more"></a><p>TODO: 待增加问题</p><ol><li>Offset怎么保存</li><li>数据倾斜怎么处理</li><li>一个Topic分配多少个Partition合适以及修改Partition的限制有哪些</li></ol><h1 id="Kafka速度很快的原因"><a href="#Kafka速度很快的原因" class="headerlink" title="Kafka速度很快的原因"></a>Kafka速度很快的原因</h1><ol><li>磁盘的顺序读写</li><li>使用操作系统的Page Cache，而不是jvm内存，这样能极大加快读写速度。避免Object对象比在Linux对象更大的消耗，避免jvm数据增多GC变慢的问题</li><li>零拷贝，Page Cache 结合 sendfile 方法，避免拷贝到用户态内存操作，Kafka消费端的性能也大幅提升。这也是为什么有时候消费端在不断消费数据时，我们并没有看到磁盘io比较高，此刻正是操作系统缓存在提供数据。</li><li>批量读写，批量压缩：把所有消息都变成一个批量文件，并且执行合理的批量压缩，减少网络IO消耗</li></ol><h1 id="如何保证消息不丢失"><a href="#如何保证消息不丢失" class="headerlink" title="如何保证消息不丢失"></a>如何保证消息不丢失</h1><h2 id="消息发布的可靠性"><a href="#消息发布的可靠性" class="headerlink" title="消息发布的可靠性"></a>消息发布的可靠性</h2><p>Producer向Topic发送消息，此时网络存在异常，producer无法得知broker是否接收到该消息，网络异常可能有两种情况：</p><ol><li>在消息传递过程中出现了网络出错</li><li>在broker已经接受到了消息，返回ack给producer的过程中网络存异常</li></ol><p>此时可以通过producer重发消息，保证at least once</p><p>0.11.0的版本通过给每个producer一个唯一ID，并且在每个消息生成一个版本号，以对消息去重，以达到producer端的exactly once</p><p>Kafka存在如下概念来保证producer端消息的可靠性</p><h3 id="TOPIC和日志"><a href="#TOPIC和日志" class="headerlink" title="TOPIC和日志"></a>TOPIC和日志</h3><p>TOPIC指的是一个订阅主题，是数据被发布的地方，可以被多个消费者订阅</p><p>对于每个主题，kafka集群都维护了一个分区日志，如下所示</p><p><img src="/images/log_anatomy.png" srcset="/img/loading.gif" alt="log_anatomy"></p><p>每个分区都是有序的，不可变的记录集，并且不断追加到结构化的commit log文件。分区中的每一个记录都会分配一个id号来表示顺序，我们称之为offset，<em>offset</em>用来唯一的标识分区中每一条记录。</p><h3 id="消费者、消费组、消息有序性"><a href="#消费者、消费组、消息有序性" class="headerlink" title="消费者、消费组、消息有序性"></a>消费者、消费组、消息有序性</h3><p>消费者使用一个 <em>消费组</em> 名称来进行标识，发布到topic中的每条记录被分配给订阅消费组中的一个消费者实例.消费者实例可以分布在多个进程中或者多个机器上。</p><p>如果所有的消费者实例在同一消费组中，消息记录会负载平衡到每一个消费者实例.</p><p>如果所有的消费者实例在不同的消费组中，每条消息记录会广播到所有的消费者进程.</p><p><img src="/images/consumer-groups.png" srcset="/img/loading.gif" alt="consumer-groups"></p><p>如图，这个 Kafka 集群有两台 server 的，四个分区(p0-p3)和两个消费者组。消费组A有两个消费者，消费组B有四个消费者。</p><p>通常情况下，每个 topic 都会有一些消费组，一个消费组对应一个”逻辑订阅者”。一个消费组由许多消费者实例组成，便于扩展和容错。这就是发布和订阅的概念，只不过订阅者是一组消费者而不是单个的进程。</p><p>在Kafka中实现消费的方式是将日志中的分区划分到每一个消费者实例上，以便在任何时间，每个实例都是分区唯一的消费者。维护消费组中的消费关系由Kafka协议动态处理。如果新的实例加入组，他们将从组中其他成员处接管一些 partition 分区;如果一个实例消失，拥有的分区将被分发到剩余的实例。</p><p>Kafka 只保证分区内的记录是有序的，而不保证主题中不同分区的顺序。每个 partition 分区按照key值排序足以满足大多数应用程序的需求。但如果你需要总记录在所有记录的上面，可使用仅有一个分区的主题来实现，这意味着每个消费者组只有一个消费者进程。</p><h3 id="分布式、分区"><a href="#分布式、分区" class="headerlink" title="分布式、分区"></a>分布式、分区</h3><p>日志的分区partition分布在kafka集群的服务器上，每个服务器在处理数据和请求时，共享这些分区，每个分区都会在已配置的服务器上进行备份，以确保容错性。</p><p>每个分区都一个server作为leader，0个或者多个server作为follower，leader server处理一切对分区的读写请求，而follower只处理被动同步leader上的数据。</p><h3 id="ACK和副本数"><a href="#ACK和副本数" class="headerlink" title="ACK和副本数"></a>ACK和副本数</h3><p>副本数保证数据高可用，ACK保证Producer接受消息策略</p><p>acks=0 ，则 producer 不会等待服务器的反馈。该方式效率最高但存在消息丢失风险</p><p>acks=1 ，leader接受到消息返回确认后，就被认为消息发布成功。但此时leader宕机存在消息丢失风险</p><p>acks=-1 ，消息在所有副本都返回成功之后才被认为消息写入成功，此种方式性能比较差，但稳定性时最高的</p><h2 id="消息接收的可靠性"><a href="#消息接收的可靠性" class="headerlink" title="消息接收的可靠性"></a>消息接收的可靠性</h2><p>暂时不支持，只能使用事务且手动提交offset方式来实现</p><h1 id="Consumer重复消费怎么处理"><a href="#Consumer重复消费怎么处理" class="headerlink" title="Consumer重复消费怎么处理"></a>Consumer重复消费怎么处理</h1><h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>笔者基于java做了一个动态添加topic，并落地数据到Hbase的功能，其他同事在复用消费topic代码做实时统计时，出现重复消费，导致统计结果不准的现象，因为写入数据到Hbase是幂等的，重复消费所以未出现问题，但是重复消费会影响到统计结果</p><h2 id="问题原因"><a href="#问题原因" class="headerlink" title="问题原因"></a>问题原因</h2><p>使用Kafka时，禁止offset自动提交，消费者每次poll的数据业务处理时间超过kafka的max.poll.interval.ms，默认是300秒，导致kafka的broker认为consumer挂掉，触发kafka进行rebalance动作，导致重新消费</p><h2 id="解决方式"><a href="#解决方式" class="headerlink" title="解决方式"></a>解决方式</h2><p>一般消费方式如下：</p><pre><code class="hljs css"><span class="hljs-selector-tag">consumer</span><span class="hljs-selector-class">.subscribe</span>(<span class="hljs-selector-tag">topicName</span>,<span class="hljs-selector-tag">rebalance</span>)<span class="hljs-selector-tag">consumer</span><span class="hljs-selector-class">.poll</span>(100)</code></pre><p>上述消费方式都会存在处理消息时长超过max.poll.interval.ms配置值风险，导致rebalance，所以最根本的解决方式，就是避免kafka进行rebalance动作，消费代码可使用如下方式</p><pre><code class="hljs dart"><span class="hljs-keyword">final</span> <span class="hljs-built_in">List</span>&lt;TopicPartition&gt; newPartitionAssignments =                <span class="hljs-keyword">new</span> ArrayList&lt;&gt;(newPartitions.size() + oldPartitionAssignmentsToPosition.size());newPartitionAssignments.addAll(oldPartitionAssignmentsToPosition.keySet());newPartitionAssignments.addAll(convertKafkaPartitions(newPartitions));<span class="hljs-comment">// reassign with the new partitions</span>consumer.assign(newPartitionAssignments);consumer.seek(topicPartition, offset)</code></pre><p>主要思想是consumer指定消费topic的对应的分区，并从指定offset进行消费，来避免kafka的rebalance动作，引起重复消费，当然这会增加消费逻辑的复杂度，需考虑很多异常情况，如consumer实例下线怎么处理，新增consumer实例，超过topic分区数怎么处理等等，可参照spark structure streaming,flink消费kafka源码实现</p><p>参考<a href="https://www.jianshu.com/p/c358a78bc92c" target="_blank" rel="noopener">https://www.jianshu.com/p/c358a78bc92c</a></p><h1 id="Kafka在分布式情况下如何保证消息的有序性"><a href="#Kafka在分布式情况下如何保证消息的有序性" class="headerlink" title="Kafka在分布式情况下如何保证消息的有序性"></a>Kafka在分布式情况下如何保证消息的有序性</h1><ol><li><p>同一个partition消息是有序的</p></li><li><p>Kafka 中发送1条消息的时候，可以指定(topic, partition, key) 3个参数</p><p>指定partition参数，则发往同一个partition的消息是有序的</p><p>指定key，具有同一个key的所有消息都会发送到一个partiton，也能够保证局部有序</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 面试 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python3下配置spark环境</title>
      <link href="/2020/07/10/Ambari%E6%98%BE%E7%A4%BAheartbeat_lost%E9%97%AE%E9%A2%98/"/>
      <url>/2020/07/10/Ambari%E6%98%BE%E7%A4%BAheartbeat_lost%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p>在/etc/profile中增加如下内容</p><pre><code class="hljs shell">export JAVA_HOME=/usr/local/java/jdk1.8.0_191export PYSPARK_PYTHON=/usr/python3/bin/python3export PYSPARK_DRIVER_PYTHON=/usr/python3/bin/python3export PYSPARK_SUBMIT_ARGS="--master local pyspark-shell"export HADOOP_USER_CLASSPATH_FIRST=trueexport PATH=$JAVA_HOME/bin:$PATHexport MAVEN_HOME=/usr/local/maven/apache-maven-3.5.4export SPARK_HOME=/usr/hdp/2.6.5.0-292/spark2export HADOOP_HOME=/usr/hdp/current/hadoop-clientexport HIVE_HOME=/usr/hdp/current/hive-server2-hive2export PATH=$SPARK_HOME/bin:$MAVEN_HOME/bin:$PATH</code></pre><p>再执行</p><p>source /etc/profile</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python3下配置spark环境</title>
      <link href="/2020/07/10/centos7%E8%BF%9E%E6%8E%A5wpa%20enterprise%E4%BC%81%E4%B8%9A%E7%BA%A7wifi/"/>
      <url>/2020/07/10/centos7%E8%BF%9E%E6%8E%A5wpa%20enterprise%E4%BC%81%E4%B8%9A%E7%BA%A7wifi/</url>
      
        <content type="html"><![CDATA[<pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> network-manager.nmcli connection edit <span class="hljs-built_in">type</span> 802-11-wireless</span><span class="hljs-meta">nmcli&gt;</span><span class="bash"> goto 802-11-wireless</span>nmcli 802-11-wireless&gt; set ssid &lt;your_ssid&gt;nmcli 802-11-wireless&gt; back<span class="hljs-meta">nmcli&gt;</span><span class="bash"> goto 802-11-wireless-security</span>nmcli 802-11-wireless-security&gt; set key-mgmt wpa-eapnmcli 802-11-wireless-security&gt; set auth-alg opennmcli 802-11-wireless-security&gt; back<span class="hljs-meta">nmcli&gt;</span><span class="bash"> goto 802-1x</span>nmcli 802-1x&gt; set eap peapnmcli 802-1x&gt; set identity &lt;your_identity&gt;nmcli 802-1x&gt; set password &lt;your_password&gt;nmcli 802-1x&gt; set phase2-auth mschapv2nmcli 802-1x&gt; back<span class="hljs-meta">nmcli&gt;</span><span class="bash"> verify</span><span class="hljs-meta">nmcli&gt;</span><span class="bash"> save</span>Saving the connection with 'autoconnect=yes'. That might result in an immediate activation of the connection.Do you still want to save? (yes/no) [yes] yesConnection 'wifi' (20e7bab0-6780-45a7-b650-eafb28e7912a) successfully saved.</code></pre><p> 保存后就自动连上了</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ambari显示heartbeat_lost问题</title>
      <link href="/2020/07/10/python3_spark%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/"/>
      <url>/2020/07/10/python3_spark%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>重启heartbeat lost机器的ambari-agent</p><h1 id="重启命令"><a href="#重启命令" class="headerlink" title="重启命令"></a>重启命令</h1><pre><code class="hljs shell">service ambari-agent start</code></pre>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>无重复最长子串</title>
      <link href="/2020/07/07/%E6%97%A0%E9%87%8D%E5%A4%8D%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/"/>
      <url>/2020/07/07/%E6%97%A0%E9%87%8D%E5%A4%8D%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2/</url>
      
        <content type="html"><![CDATA[<h4 id="3-无重复字符的最长子串"><a href="#3-无重复字符的最长子串" class="headerlink" title="3. 无重复字符的最长子串"></a><a href="https://leetcode-cn.com/problems/longest-substring-without-repeating-characters/" target="_blank" rel="noopener">3. 无重复字符的最长子串</a></h4><p>给定一个字符串，请你找出其中不含有重复字符的 <strong>最长子串</strong> 的长度。</p><a id="more"></a><p><strong>示例 1:</strong></p><pre><code class="hljs makefile"><span class="hljs-section">输入: "abcabcbb"</span><span class="hljs-section">输出: 3 </span><span class="hljs-section">解释: 因为无重复字符的最长子串是 "abc"，所以其长度为 3。</span></code></pre><p><strong>示例 2:</strong></p><pre><code class="hljs makefile"><span class="hljs-section">输入: "bbbbb"</span><span class="hljs-section">输出: 1</span><span class="hljs-section">解释: 因为无重复字符的最长子串是 "b"，所以其长度为 1。</span></code></pre><p><strong>示例 3:</strong></p><pre><code class="hljs makefile"><span class="hljs-section">输入: "pwwkew"</span><span class="hljs-section">输出: 3</span><span class="hljs-section">解释: 因为无重复字符的最长子串是 "wke"，所以其长度为 3。</span>     请注意，你的答案必须是 子串 的长度，<span class="hljs-string">"pwke"</span> 是一个子序列，不是子串。</code></pre><h1 id="首次代码"><a href="#首次代码" class="headerlink" title="首次代码"></a>首次代码</h1><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.bd.leetcode.leetcode_3;<span class="hljs-keyword">import</span> java.util.*;<span class="hljs-comment">/**</span><span class="hljs-comment"> * <span class="hljs-doctag">@author</span> </span><span class="hljs-comment"> * <span class="hljs-doctag">@date</span>: 2020/7/7</span><span class="hljs-comment"> */</span><span class="hljs-comment">/**</span><span class="hljs-comment"> * 1.该题可以用动态划块解决，startIndex表示划块起始位置，endIndex为结束为止</span><span class="hljs-comment"> * 2.stringBuffer为动态划块中字符串的内容</span><span class="hljs-comment"> * 3.每次循环endIndex加1，如果stringBuffer包含sArr[endIndex]，使用for循环查找最后一次该值在sArr中出现的位置，并将startIndex+1作为该值的起始位（substr时候去掉该值）</span><span class="hljs-comment"> */</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span> </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> <span class="hljs-title">lengthOfLongestSubstring</span><span class="hljs-params">(String s)</span> </span>&#123;    String aStr = s;    <span class="hljs-keyword">char</span>[] sArr = aStr.toCharArray();    StringBuffer stringBuffer = <span class="hljs-keyword">new</span> StringBuffer();    String max = <span class="hljs-keyword">null</span>;    <span class="hljs-keyword">int</span> maxLength = <span class="hljs-number">0</span>;    <span class="hljs-keyword">if</span> (<span class="hljs-number">0</span> == sArr.length) &#123;      maxLength = <span class="hljs-number">0</span>;    &#125;    <span class="hljs-keyword">if</span> (<span class="hljs-number">1</span> == sArr.length) &#123;      maxLength = <span class="hljs-number">1</span>;    &#125;    <span class="hljs-keyword">int</span> startIndex = <span class="hljs-number">0</span>;    <span class="hljs-keyword">int</span> endIndex = <span class="hljs-number">0</span>;    <span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>;    <span class="hljs-keyword">while</span> (i &lt; sArr.length) &#123;      <span class="hljs-keyword">if</span> (!stringBuffer.toString().contains(sArr[i] + <span class="hljs-string">""</span>)) &#123; <span class="hljs-comment">//不包含sArr[endIndex]，增加划块长度</span>        endIndex++;        <span class="hljs-keyword">if</span> (endIndex - startIndex &gt; maxLength) &#123;          maxLength = endIndex - startIndex;          max = aStr.substring(startIndex, endIndex);        &#125;        stringBuffer.delete(<span class="hljs-number">0</span>, stringBuffer.length()).append(aStr.substring(startIndex, endIndex));      &#125; <span class="hljs-keyword">else</span> &#123;<span class="hljs-comment">//包含startIndex增加，直到sArr[i]不再被包含</span>        <span class="hljs-keyword">while</span> (endIndex &lt;= i &amp;&amp; startIndex &lt; endIndex) &#123; <span class="hljs-comment">//endIndex表示新加入的char，用循环获取最后一次出现的位置</span>          <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = endIndex - <span class="hljs-number">1</span>; j &gt;= <span class="hljs-number">0</span>; j--) &#123;            <span class="hljs-keyword">if</span> (sArr[j] == sArr[endIndex]) &#123;              startIndex = j + <span class="hljs-number">1</span>;              <span class="hljs-keyword">break</span>;            &#125;          &#125;          endIndex++;          stringBuffer.delete(<span class="hljs-number">0</span>, stringBuffer.length()).append(aStr.substring(startIndex, endIndex));        &#125;      &#125;      i++;    &#125;    <span class="hljs-comment">//最长连续</span>    <span class="hljs-comment">//System.out.println(max);</span>    <span class="hljs-keyword">return</span> maxLength;  &#125;  <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;    Solution s = <span class="hljs-keyword">new</span> Solution();    <span class="hljs-keyword">int</span> len = s.lengthOfLongestSubstring(<span class="hljs-string">"aab"</span>);    System.out.println(len);  &#125;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常见OOM及原因分析</title>
      <link href="/2020/07/06/OOM%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"/>
      <url>/2020/07/06/OOM%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/</url>
      
        <content type="html"><![CDATA[<h1 id="常见OOM类型"><a href="#常见OOM类型" class="headerlink" title="常见OOM类型"></a>常见OOM类型</h1><h2 id="堆溢出"><a href="#堆溢出" class="headerlink" title="堆溢出"></a>堆溢出</h2><p>错误信息：java.lang.OutOfMemoryError: Java heap space </p><p>Java堆用于存储对象实例，只要不断地创建对象，并且保证GC Roots到对象之间有可达路径来避免垃圾回收机制清除这些对象，那么在对象数量到达最大堆的容量限制后就会产生内存溢出异常。 </p><a id="more"></a><h3 id="Java-堆溢出排查解决思路"><a href="#Java-堆溢出排查解决思路" class="headerlink" title="Java 堆溢出排查解决思路"></a>Java 堆溢出排查解决思路</h3><ol><li>查找关键报错信息，如</li></ol><pre><code class="hljs java">java.lang.OutOfMemoryError: Java heap space</code></pre><ol><li>使用内存映像分析工具（如Eclipsc Memory Analyzer或者Jprofiler）对Dump出来的堆储存快照进行分析，分析清楚是内存泄漏还是内存溢出。</li><li>如果是内存泄漏，可进一步通过工具查看泄漏对象到GC    Roots的引用链，修复应用程序中的内存泄漏。</li><li>如果不存在泄漏，先检查代码是否有死循环，递归等，再考虑用 -Xmx 增加堆大小。</li></ol><h2 id="栈溢出"><a href="#栈溢出" class="headerlink" title="栈溢出"></a>栈溢出</h2><pre><code class="hljs java">java.lang.OutOfMemoryError: unable to create <span class="hljs-keyword">new</span> <span class="hljs-keyword">native</span> thread</code></pre><p>关于虚拟机栈和本地方法栈，在Java虚拟机规范中描述了两种异常：</p><ul><li>如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError 异常；</li><li>如果虚拟机栈可以动态扩展，当扩展时无法申请到足够的内存时会抛出 OutOfMemoryError 异常。</li></ul><h3 id="栈溢出原因"><a href="#栈溢出原因" class="headerlink" title="栈溢出原因"></a>栈溢出原因</h3><ul><li>在单个线程下，栈帧太大，或者虚拟机栈容量太小，当内存无法分配的时候，虚拟机抛出StackOverflowError 异常。</li><li>不断地建立线程的方式会导致内存溢出</li></ul><h2 id="方法区溢出"><a href="#方法区溢出" class="headerlink" title="方法区溢出"></a>方法区溢出</h2><pre><code class="hljs JAVA">java.lang.OutOfMemoryError: Metaspace</code></pre><p> 方法区，（又叫永久代，JDK8后，元空间替换了永久代），用于存放Class的相关信息，如类名、访问修饰符、常量池、字段描述、方法描述等。运行时产生大量的类，会填满方法区，造成溢出。 </p><h3 id="方法区溢出原因"><a href="#方法区溢出原因" class="headerlink" title="方法区溢出原因"></a>方法区溢出原因</h3><ul><li>使用CGLib生成了大量的代理类，导致方法区被撑爆</li><li>在Java7之前，频繁的错误使用String.intern方法</li><li>大量jsp和动态产生jsp</li><li>应用长时间运行，没有重启</li></ul><h3 id="方法区溢出排查解决思路"><a href="#方法区溢出排查解决思路" class="headerlink" title="方法区溢出排查解决思路"></a>方法区溢出排查解决思路</h3><ul><li>检查是否永久代空间设置得过小</li><li>检查代码是否频繁错误得使用String.intern方法</li><li>检查是否跟jsp有关。</li><li>检查是否使用CGLib生成了大量的代理类</li><li>重启大法，重启JVM</li></ul><h2 id="本机直接内存溢出"><a href="#本机直接内存溢出" class="headerlink" title="本机直接内存溢出"></a>本机直接内存溢出</h2><pre><code class="hljs java">java.lang.OutOfMemoryError: Direct buffer memory</code></pre><p>直接内存并不是虚拟机运行时数据区的一部分，也不是Java 虚拟机规范中定义的内存区域。但是，这部分内存也被频繁地使用，而且也可能导致OOM。</p><p>在JDK1.4 中新加入了NIO(New Input/Output)类，它可以使用 native 函数库直接分配堆外内存，然后通过一个存储在Java堆中的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样能在一些场景中显著提高性能，因为避免了在 Java 堆和 Native 堆中来回复制数据。</p><h3 id="直接内存溢出原因"><a href="#直接内存溢出原因" class="headerlink" title="直接内存溢出原因"></a>直接内存溢出原因</h3><ul><li>本机直接内存的分配虽然不会受到Java 堆大小的限制，但是受到本机总内存大小限制。</li><li>直接内存由 -XX:MaxDirectMemorySize 指定，如果不指定，则默认与Java堆最大值（-Xmx指定）一样。</li><li>NIO程序中，使用ByteBuffer.allocteDirect(capability)分配的是直接内存，可能导致直接内存溢出。</li></ul><h3 id="直接内存溢出"><a href="#直接内存溢出" class="headerlink" title="直接内存溢出"></a>直接内存溢出</h3><ul><li>检查代码是否恰当</li><li>检查JVM参数-Xmx，-XX:MaxDirectMemorySize 是否合理</li></ul><h2 id="GC-overhead-limit-exceeded"><a href="#GC-overhead-limit-exceeded" class="headerlink" title="GC overhead limit exceeded"></a>GC overhead limit exceeded</h2><pre><code class="hljs JAVA">java.lang.OutOfMemoryError: GC overhead limit exceeded</code></pre><ul><li>这个是JDK6新加的错误类型，一般都是堆太小导致的。</li><li>Sun 官方对此的定义：超过98%的时间用来做GC并且回收了不到2%的堆内存时会抛出此异常。</li></ul><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><ul><li>检查项目中是否有大量的死循环或有使用大内存的代码，优化代码。</li><li>检查JVM参数-Xmx -Xms是否合理</li><li>dump内存，检查是否存在内存泄露，如果没有，加大内存。</li></ul><h1 id="OOM常见原因分析"><a href="#OOM常见原因分析" class="headerlink" title="OOM常见原因分析"></a>OOM常见原因分析</h1><p>Java服务出现OOM，最常见的原因是：</p><ol><li>内存确实分配过小，内存确实不够用；</li><li>某一个对象被频繁申请，却没有释放，内存不断泄漏，导致内存耗尽；</li><li>某一个资源被频繁申请，系统资源耗尽，例如：不断创建线程，不断发起网络连接；更具体的，可以按照以下步骤，使用以下工具排查。</li></ol><h2 id="确认是不是内存本身就分配过小"><a href="#确认是不是内存本身就分配过小" class="headerlink" title="确认是不是内存本身就分配过小"></a><strong>确认是不是内存本身就分配过小</strong></h2><p> jmap -heap 10765 </p><p> <img src="/images/oom.png" srcset="/img/loading.gif" alt="image"> </p><p> 如上图，可以查看新生代，老生代堆内存的分配大小以及使用情况，看是否本身分配过小。 </p><h2 id="找到最耗内存的对象"><a href="#找到最耗内存的对象" class="headerlink" title="找到最耗内存的对象"></a>找到最耗内存的对象</h2><p> jmap -histo:live 10765 | more </p><p> <img src="/images/OOM2.png" srcset="/img/loading.gif" alt="image"> </p><p> 需要说明的是，jmap -histo:live 会执行一次FGC，如果仍无法定位，可dump内存，通过Java内存分析工具MAT<strong>（Memory Analyzer Tool</strong>）线下进行分析。 </p><p> 上图中占内存最多的对象是RingBufferLogEvent，共占用内存18M，属于正常使用范围。如果发现某类对象占用内存很大（例如几个G），很可能是类对象创建太多，且一直未释放 </p><p>例如：<br>1.申请完资源后，未调用close()或dispose()释放资源；<br>2.消费者消费速度慢（或停止消费了），而生产者不断往队列中投递任务，导致队列中任务累积过多； </p><h2 id="确认是否是资源耗尽工具"><a href="#确认是否是资源耗尽工具" class="headerlink" title="确认是否是资源耗尽工具"></a><strong>确认是否是资源耗尽</strong>工具</h2><ol><li>pstree </li><li>netstat </li></ol><p>查看进程创建的线程数，以及网络连接数，如果资源耗尽，也可能出现OOM。这里介绍另一种方法，通过/proc/${PID}/fd 和 /proc/${PID}/task </p><p> <img src="/images/oom3.png" srcset="/img/loading.gif" alt="image"> </p><p>如上图，sshd共占用了四个句柄<br>（1）0 -&gt; 标准输入；<br>（2）1 -&gt; 标准输出；<br>（3）2 -&gt; 标准错误输出；<br>（4）3 -&gt; socket（容易想到是监听端口）；</p><p>sshd只有一个主线程PID为9339，并没有多线程。所以，只要</p><p>ll /proc/${PID}/fd | wc -l<br>ll /proc/${PID}/task | wc -l （效果等同pstree -p | wc -l）<br>就能知道进程打开的句柄数和线程数。</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jvm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark专项训练</title>
      <link href="/2020/07/06/Spark%E6%A6%82%E5%BF%B5/"/>
      <url>/2020/07/06/Spark%E6%A6%82%E5%BF%B5/</url>
      
        <content type="html"><![CDATA[<h1 id="Spark部署模式"><a href="#Spark部署模式" class="headerlink" title="Spark部署模式"></a>Spark部署模式</h1><ol><li>standalone模式，开启7077端口提供服务</li><li>spark on yarn模式 ：<ol><li>client 模式， driver运行在客户端，调试用 </li><li>cluster模式， 分布式运行，driver运行在集群子节点 </li></ol></li></ol><a id="more"></a><h1 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h1><h2 id="什么是RDD"><a href="#什么是RDD" class="headerlink" title="什么是RDD"></a>什么是RDD</h2><p>弹性分布式数据集（RDD），Spark中的基本抽象。 </p><p>代表着一种不可变的，可以被并行操作的集合， 这个类包含RDD所有的基本操作，例如map,filter,perssist</p><h2 id="RDD有什么属性"><a href="#RDD有什么属性" class="headerlink" title="RDD有什么属性"></a>RDD有什么属性</h2><p> 一组分片</p><p> 一个计算每个分区的函数 </p><p> RDD之间的依赖关系 </p><p> 一个Partitioner，即RDD的分片函数。 </p><p> 一个列表，存储存取每个Partition的优先位置（preferred location） </p><h2 id="RDD弹性表现在那哪些方面"><a href="#RDD弹性表现在那哪些方面" class="headerlink" title="RDD弹性表现在那哪些方面"></a>RDD弹性表现在那哪些方面</h2><p> 自动进行磁盘和内存存储的切换 </p><p> 基于lineage的高效容错 </p><p> task执行失败会进行重试 </p><p> stage执行失败会进行重试，并且只重试失败的分片 </p><p> checkpoint和persist数据的持久化缓存 </p><h2 id="RDD的宽依赖窄依赖，stage划分"><a href="#RDD的宽依赖窄依赖，stage划分" class="headerlink" title="RDD的宽依赖窄依赖，stage划分"></a>RDD的宽依赖窄依赖，stage划分</h2><p>窄依赖： 窄依赖就是指父RDD的每个分区只被一个子RDD分区使用 </p><p>宽依赖： 宽依赖就是指父RDD的每个分区都有可能被多个子RDD分区使用， 宽依赖（shuffle）由于依赖的上游RDD不止一个，所以往往需要跨节点传输数据。 </p><p>stage： 窄依赖会被划分到同一个Stage中，这样它们就能以管道的方式迭代执行 ， 宽依赖往往对应着shuffle操作，当执行算子有shuffle操作的时候，就划分一个Stage，宽依赖是划分stage的依据 </p><p>stage容灾： 窄依赖只需要重新执行父RDD的丢失分区的计算即可恢复。 宽依赖则需要考虑恢复所有父RDD的丢失分区，并且同一RDD下的其他分区数据也重新计算了一次。  ，</p><h2 id="RDD持久化"><a href="#RDD持久化" class="headerlink" title="RDD持久化"></a>RDD持久化</h2><h3 id="Cache"><a href="#Cache" class="headerlink" title="Cache"></a>Cache</h3><p>cache是persist(STORAGE_LEVEL=MEMORY_ONLY) </p><h3 id="Persist持久化策略"><a href="#Persist持久化策略" class="headerlink" title="Persist持久化策略"></a>Persist持久化策略</h3><p>MEMORY_ONLY ： 使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍。这是默认的持久化策略，使用cache()方法时，实际就是使用的这种持久化策略。 </p><p>MEMORY_ONLY_SER： 基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。 </p><p>MEMORY_AND_DISK ： 使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。 </p><p>MEMORY_AND_DISK_SER ： 基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。 </p><p>DISK_ONLY： 使用未序列化的Java对象格式，将数据全部写入磁盘文件中。 </p><p>MEMORY_ONLY_2,MEMORY_AND_DISK_2 ： 对于上述任意一种持久化策略，如果加上后缀_2，代表的是将每个持久化的数据，都复制一份副本，并将副本保存到其他节点上。这种基于副本的持久化机制主要用于进行容错。假如某个节点挂掉，节点的内存或磁盘中的持久化数据丢失了，那么后续对RDD计算时还可以使用该数据在其他节点上的副本。如果没有副本的话，就只能将这些数据从源头处重新计算一遍了。 </p><h3 id="Checkpoint"><a href="#Checkpoint" class="headerlink" title="Checkpoint"></a>Checkpoint</h3><h4 id="哪些-RDD-需要-checkpoint？"><a href="#哪些-RDD-需要-checkpoint？" class="headerlink" title="哪些 RDD 需要 checkpoint？"></a>哪些 RDD 需要 checkpoint？</h4><p> 运算时间很长或运算量太大才能得到的 RDD 或是计算链过长或依赖其他 RDD 很多的 RDD </p><h4 id="什么时候进行checkpoint"><a href="#什么时候进行checkpoint" class="headerlink" title="什么时候进行checkpoint"></a>什么时候进行checkpoint</h4><p>cache 机制是每计算出一个要 cache 的 partition 就直接将其 cache 到内存了。但 checkpoint 没有类似的方法，而是等到 job 结束后另外启动专门的 job 去完成 checkpoint 。  </p><p>因此 checkpoint 的 RDD 会被计算两次。因此，在使用 rdd.checkpoint() 的时候，建议在该语句前面加上 rdd.cache()，这样第二次运行的 job 就不用再去计算该 rdd 了，直接读取 cache 写磁盘。 </p><h4 id="Checkpoint和Cache，Persist区别"><a href="#Checkpoint和Cache，Persist区别" class="headerlink" title="Checkpoint和Cache，Persist区别"></a>Checkpoint和Cache，Persist区别</h4><h5 id="Checkpoint与Cache的区别"><a href="#Checkpoint与Cache的区别" class="headerlink" title="Checkpoint与Cache的区别"></a>Checkpoint与Cache的区别</h5><p>cache把 RDD 计算出来然后放在内存中， 但是RDD 的依赖链也不能丢掉， 当某个点某个 executor 宕了， 上面cache 的RDD就会丢掉， 需要通过依赖链重新计算出来；</p><p>checkpoint 是把 RDD 保存在 HDFS中， 是多副本可靠存储，所以依赖链就可以丢掉了，就斩断了依赖链，因为checkpoint是需要把 job 重新从头算一遍， 最好先cache一下， checkpoint就可以直接保存缓存中的 RDD 了， 就不需要重头计算一遍了， 对性能有极大的提升。 </p><h5 id="Checkpoint与Persist区别"><a href="#Checkpoint与Persist区别" class="headerlink" title="Checkpoint与Persist区别"></a>Checkpoint与Persist区别</h5><p>rdd.persist(StorageLevel.DISK_ONLY) 与 checkpoint 区别的是：前者虽然可以将 RDD 的 partition 持久化到磁盘，但该 partition 由 blockManager 管理。一旦 driver program 执行结束，也就是 executor 所在进程 CoarseGrainedExecutorBackend stop，blockManager 也会 stop，被 cache 到磁盘上的 RDD 也会被清空（整个 blockManager 使用的 local 文件夹被删除）。而 checkpoint 将 RDD 持久化到 HDFS 或本地文件夹，如果不被手动 remove 掉（ 话说怎么 remove checkpoint 过的 RDD？ ），是一直存在的，也就是说可以被下一个 driver program 使用，而 cached RDD 不能被其他 dirver program 使用。 </p><h1 id="Spark工作流"><a href="#Spark工作流" class="headerlink" title="Spark工作流"></a>Spark工作流</h1><h2 id="提交任务之后发生了什么"><a href="#提交任务之后发生了什么" class="headerlink" title="提交任务之后发生了什么"></a>提交任务之后发生了什么</h2><ol><li><p>构建Spark Application的运行环境（启动SparkContext） </p></li><li><p>SparkContext向资源管理器（可以是Standalone、Mesos或YARN）注册并申请运行Executor资源； </p></li><li><p>资源管理器分配Executor资源，Executor运行情况将随着心跳发送到资源管理器上；（yarn会分配worker上的资源，worker将运行情况随心跳发送给executor） </p></li><li><p>SparkContext构建成DAG图，将DAG图分解成Stage，并把Taskset发送给Task Scheduler </p></li><li><p>Executor向SparkContext申请Task，Task Scheduler将Task发放给Executor运行，SparkContext将应用程序代码发放给Executor。 </p></li><li><p>Task在Executor上运行，运行完毕释放所有资源。 </p></li></ol><h2 id="Spark组件作用"><a href="#Spark组件作用" class="headerlink" title="Spark组件作用"></a>Spark组件作用</h2><p>master : 管理节点不参与运算 </p><p>worker : 分配任务给executor，向master汇报资源使用情况 </p><p>driver : 一个Spark作业运行时包括一个Driver进程，也是作业的主进程，具有main函数，并且有SparkContext的实例，是程序的人口点 , 作用：向集群申请资源，向master注册信息，负责作业调度（生成stage层，并将task任务分配到executor上） </p><p>sparkContext : 向yarn申请资源 </p><p>client : 提交程序的入口 </p><h2 id="对于-Spark-中的数据倾斜问题你有什么好的方案？"><a href="#对于-Spark-中的数据倾斜问题你有什么好的方案？" class="headerlink" title="对于 Spark 中的数据倾斜问题你有什么好的方案？"></a>对于 Spark 中的数据倾斜问题你有什么好的方案？</h2><h3 id="什么是数据倾斜"><a href="#什么是数据倾斜" class="headerlink" title="什么是数据倾斜"></a>什么是数据倾斜</h3><p>对 Spark/Hadoop 这样的大数据系统来讲，数据量大并不可怕，可怕的是数据倾斜。数据倾斜指的是，并行处理的数据集中，某一部分（如 Spark 或 Kafka 的一个 Partition）的数据显著多于其它部分，从而使得该部分的处理速度成为整个数据集处理的瓶颈（木桶效应）。 </p><h3 id="数据倾斜是如何造成的"><a href="#数据倾斜是如何造成的" class="headerlink" title="数据倾斜是如何造成的"></a>数据倾斜是如何造成的</h3><p>某个stage中，包含N个task，前N-1个任务执行耗时很短，第N个执行耗时很长，这样导致无法很好利用并行，造成所有任务都在等第N个任务执行完成 </p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><h4 id="调整并行度，发生数据倾斜的任务分成多个任务并行执行"><a href="#调整并行度，发生数据倾斜的任务分成多个任务并行执行" class="headerlink" title="调整并行度，发生数据倾斜的任务分成多个任务并行执行"></a>调整并行度，发生数据倾斜的任务分成多个任务并行执行</h4><p>Spark 在做 Shuffle 时，默认使用 HashPartitioner对数据进行分区。如果并行度设置的不合适，可能造成大量不相同的 Key 对应的数据被分配到了同一个 Task 上，造成该 Task 所处理的数据远大于其它 Task，从而造成数据倾斜。</p><h4 id="自定义Partitioner"><a href="#自定义Partitioner" class="headerlink" title="自定义Partitioner"></a>自定义Partitioner</h4><p>使用自定义的 Partitioner（默认为 HashPartitioner），将原本被分配到同一个 Task 的不同 Key 分配到不同 Task </p><h4 id="将-Reduce-side（侧）-Join-转变为-Map-side（侧）-Join"><a href="#将-Reduce-side（侧）-Join-转变为-Map-side（侧）-Join" class="headerlink" title="将 Reduce side（侧） Join 转变为 Map side（侧） Join"></a>将 Reduce side（侧） Join 转变为 Map side（侧） Join</h4><p>通过 Spark 的 Broadcast 机制，将 Reduce 侧 Join 转化为 Map 侧 Join，避免 Shuffle 从而完全消除 Shuffle 带来的数据倾斜。 </p><h4 id="为数据量特别大的-Key-增加随机前-后缀"><a href="#为数据量特别大的-Key-增加随机前-后缀" class="headerlink" title="为数据量特别大的 Key 增加随机前/后缀"></a>为数据量特别大的 Key 增加随机前/后缀</h4><p>为数据量特别大的 Key 增加随机前/后缀，使得原来 Key 相同的数据变为 Key 不相同的数据，从而使倾斜的数据集分散到不同的 Task 中，彻底解决数据倾斜问题。Join 另一则的数据中，与倾斜 Key 对应的部分数据，与随机前缀集作笛卡尔乘积，从而保证无论数据倾斜侧倾斜 Key 如何加前缀，都能与之正常 Join。 </p><h1 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle"></a>Shuffle</h1><h2 id="什么是Shuffle"><a href="#什么是Shuffle" class="headerlink" title="什么是Shuffle"></a>什么是Shuffle</h2><p>某种具有共同特征的数据汇聚到一个计算节点上进行计算</p><p>另一种说法： 将相同的 Key 分发至同一个 Reducer上进行处理 </p><h2 id="如何避免shuffle"><a href="#如何避免shuffle" class="headerlink" title="如何避免shuffle"></a>如何避免shuffle</h2><p>能避免则尽可能避免使用 reduceByKey、join、distinct、repartition 等会进行 shuffle 的算子, 尽量使用 map 类的非 shuffle 算子 </p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>三数之和</title>
      <link href="/2020/07/06/%E4%B8%89%E6%95%B0%E7%9B%B8%E5%8A%A0/"/>
      <url>/2020/07/06/%E4%B8%89%E6%95%B0%E7%9B%B8%E5%8A%A0/</url>
      
        <content type="html"><![CDATA[<h4 id="15-三数之和"><a href="#15-三数之和" class="headerlink" title="15. 三数之和"></a><a href="https://leetcode-cn.com/problems/3sum/" target="_blank" rel="noopener">15. 三数之和</a></h4><p> 给你一个包含 <em>n</em> 个整数的数组 <code>nums</code>，判断 <code>nums</code> 中是否存在三个元素 <em>a，b，c ，*使得 *a + b + c =</em> 0 ？请你找出所有满足条件且不重复的三元组。</p><p><strong>注意：</strong>答案中不可以包含重复的三元组。</p><p><strong>示例：</strong></p><pre><code class="hljs angelscript">给定数组 nums = [<span class="hljs-number">-1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">-1</span>, <span class="hljs-number">-4</span>]，满足要求的三元组集合为：[  [<span class="hljs-number">-1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>],  [<span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>, <span class="hljs-number">2</span>]]</code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink窗口</title>
      <link href="/2020/07/03/Flink%E7%AA%97%E5%8F%A3/"/>
      <url>/2020/07/03/Flink%E7%AA%97%E5%8F%A3/</url>
      
        <content type="html"><![CDATA[<h1 id="什么是窗口"><a href="#什么是窗口" class="headerlink" title="什么是窗口"></a>什么是窗口</h1><p>Windows是处理流试计算的核心。 Windows将流分成有限个大小的“存储桶”，我们可以在“存储桶”上应用计算。</p><h1 id="窗口类型"><a href="#窗口类型" class="headerlink" title="窗口类型"></a>窗口类型</h1><h2 id="Tumbling-Window"><a href="#Tumbling-Window" class="headerlink" title="Tumbling Window"></a>Tumbling Window</h2><p>翻滚窗口，无数据重叠</p><p>滚动窗口分配器将每个元素分配给指定窗口大小的窗口。 滚动窗口具有固定的大小，并且不重叠。 例如，如果您指定大小为5分钟的翻滚窗口，则将评估当前窗口，并且每五分钟将启动一个新窗口，如下图所示。</p><a id="more"></a><p><img src="/images/tumbling-windows.svg" srcset="/img/loading.gif" alt="tumbling-windows"></p><h2 id="Sliding-Window"><a href="#Sliding-Window" class="headerlink" title="Sliding Window"></a>Sliding Window</h2><p>滑动窗口，可数据重叠</p><p>滑动窗口分配器将元素分配给固定长度的窗口。和滚动窗口相比，窗口大小可以被windows param参数控制，一个额外的滑动参数控制滑动窗口的滑动频率。因此滑动窗口是可以重复的当滑动大小小于窗口大小。在这种情况下，元素可以被分配给多个窗口（可以重复）</p><p>例如：你可以存在一组长度为10分钟的窗口集合，这样你可以每隔5分钟就得到下一个窗口，且包含最近10分钟的所有事件，示意图如下所示</p><p><img src="/images/sliding-windows.svg" srcset="/img/loading.gif" alt="sliding-windows"></p><h2 id="Session-Window"><a href="#Session-Window" class="headerlink" title="Session Window"></a>Session Window</h2><p>Session Window，活动时间间隙相等的窗口</p><p>会话窗口分配器按活动会话对元素进行分组。与滚动窗口和滑动窗口相比，会话窗口不重叠且没有固定的开始和结束时间。 相反，当会话窗口在一定时间段内未接收到元素时（即，发生不活动间隙时），它将关闭。 会话窗口分配器可以配置有静态会话间隔，也可以配置有会话间隔提取器功能，该功能定义不活动的时间长度。 当该时间段到期时，当前会话将关闭，随后的元素将分配给新的会话窗口。</p><p><img src="/images/session-windows.svg" srcset="/img/loading.gif" alt="sliding-windows"></p><h2 id="Global-Windows"><a href="#Global-Windows" class="headerlink" title="Global Windows"></a>Global Windows</h2><p>Global Windows 全局窗口</p><p>全局窗口分配器将具有相同键的所有元素分配给同一单个全局窗口。 仅当您还指定自定义触发器时，此窗口方案才有用。 否则，将不会执行任何计算，因为全局窗口没有可以处理聚合元素的自然终点。</p><h1 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h1><p>在定义窗口分配器后，我们需要指定每个窗口上执行的计算。一旦系统处于就绪状态，就可以处理每个（可能是键控）窗口中的元素，这是窗口函数的职能所在（请参阅Flink如何确定窗口何时准备就绪的触发器）</p><p>窗口函数可以是ReduceFunction, AggregateFunction, FoldFunction or ProcessWindowFunction中的一种，前两个性能更高，因为在每个窗口中元素到达的时候flink是增量聚合的。一个ProcessWindowFunction函数可以为窗口中的元素获取一个迭代器，该迭代器包含一个窗口且包含窗口中元素的额外信息</p><p>使用ProcessWindowFunction进行窗口转换不能像其他情况一样有效地执行，因为Flink必须在调用函数之前在内部缓冲窗口的所有元素。可以通过将ProcessWindowFunction与ReduceFunction，AggregateFunction或FoldFunction组合使用来获得窗口元素的增量聚合以及ProcessWindowFunction接收的其他窗口元数据，从而缓解这种情况。代码示例如下。</p><h2 id="ReduceFunction"><a href="#ReduceFunction" class="headerlink" title="ReduceFunction"></a>ReduceFunction</h2><p>一个ReduceFunction函数指定了如何将输入中的两个元素组合，且输出一个相同类型的结果。Flink使用ReduceFunction来增量聚合窗口中的元素。</p><p>一个ReduceFunction函数可以这样来使用</p><pre><code class="hljs java">DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;input    .keyBy(&lt;key selector&gt;)    .window(&lt;window assigner&gt;)    .reduce(<span class="hljs-keyword">new</span> ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt; &#123;      <span class="hljs-function"><span class="hljs-keyword">public</span> Tuple2&lt;String, Long&gt; <span class="hljs-title">reduce</span><span class="hljs-params">(Tuple2&lt;String, Long&gt; v1, Tuple2&lt;String, Long&gt; v2)</span> </span>&#123;        <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> Tuple2&lt;&gt;(v1.f0, v1.f1 + v2.f1);      &#125;    &#125;);</code></pre><p>上面的例子操作是将Tuple第二个位置上的元素相加</p><h2 id="AggregateFunction"><a href="#AggregateFunction" class="headerlink" title="AggregateFunction"></a>AggregateFunction</h2><p>AggregateFunction是ReduceFunction的通用版本，具有三种类型：输入类型（IN），累加器类型（ACC）和输出类型（OUT）。输入类型是输入流中元素的类型，AggregateFunction具有一种将一个输入元素添加到累加器的方法。 该接口还具有创建初始累加器，将两个累加器合并为一个累加器以及从累加器提取输出（OUT类型）的方法。 我们将在下面的示例中看到它的工作原理。</p><p>与ReduceFunction相同，Flink将在窗口的输入元素到达时对其进行增量聚合。</p><pre><code class="hljs java"><span class="hljs-comment">/**</span><span class="hljs-comment"> * 计算窗口中元素的第二个字段的平均值。</span><span class="hljs-comment"> */</span><span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">AverageAggregate</span></span><span class="hljs-class">         <span class="hljs-keyword">implements</span> <span class="hljs-title">AggregateFunction</span>&lt;<span class="hljs-title">Tuple2</span>&lt;<span class="hljs-title">String</span>, <span class="hljs-title">Long</span>&gt;, <span class="hljs-title">Tuple2</span>&lt;<span class="hljs-title">Long</span>, <span class="hljs-title">Long</span>&gt;, <span class="hljs-title">Double</span>&gt; </span>&#123;    <span class="hljs-meta">@Override</span>  <span class="hljs-function"><span class="hljs-keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="hljs-title">createAccumulator</span><span class="hljs-params">()</span> </span>&#123;    <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> Tuple2&lt;&gt;(<span class="hljs-number">0L</span>, <span class="hljs-number">0L</span>);  &#125;  <span class="hljs-meta">@Override</span>  <span class="hljs-function"><span class="hljs-keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="hljs-title">add</span><span class="hljs-params">(Tuple2&lt;String, Long&gt; value, Tuple2&lt;Long, Long&gt; accumulator)</span></span>&#123;    <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> Tuple2&lt;&gt;(accumulator.f0 + value.f1, accumulator.f1 + <span class="hljs-number">1L</span>);  &#125;  <span class="hljs-meta">@Override</span>  <span class="hljs-function"><span class="hljs-keyword">public</span> Double <span class="hljs-title">getResult</span><span class="hljs-params">(Tuple2&lt;Long, Long&gt; accumulator)</span> </span>&#123;    <span class="hljs-keyword">return</span> ((<span class="hljs-keyword">double</span>) accumulator.f0) / accumulator.f1;  &#125;  <span class="hljs-meta">@Override</span>  <span class="hljs-function"><span class="hljs-keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="hljs-title">merge</span><span class="hljs-params">(Tuple2&lt;Long, Long&gt; a, Tuple2&lt;Long, Long&gt; b)</span> </span>&#123;    <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> Tuple2&lt;&gt;(a.f0 + b.f0, a.f1 + b.f1);  &#125;&#125;DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;input    .keyBy(&lt;key selector&gt;)    .window(&lt;window assigner&gt;)    .aggregate(<span class="hljs-keyword">new</span> AverageAggregate());</code></pre><h2 id="FoldFunction"><a href="#FoldFunction" class="headerlink" title="FoldFunction"></a>FoldFunction</h2><h2 id="ProcessWindowFunction"><a href="#ProcessWindowFunction" class="headerlink" title="ProcessWindowFunction"></a>ProcessWindowFunction</h2><p>ProcessWindowFunction获取一个Iterable，该Iterable包含窗口的所有元素，以及一个Context对象，该对象可以访问时间和状态信息，从而使其比其他窗口函数更具灵活性。 这是以性能和资源消耗为代价的，因为无法增量聚合元素，而是需要在内部对其进行缓冲，直到将窗口视为已准备好进行处理为止。</p><pre><code class="hljs java">DataStream&lt;Tuple2&lt;String, Long&gt;&gt; input = ...;input  .keyBy(t -&gt; t.f0)  .timeWindow(Time.minutes(<span class="hljs-number">5</span>))  .process(<span class="hljs-keyword">new</span> MyProcessWindowFunction());<span class="hljs-comment">/* ... */</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyProcessWindowFunction</span> </span><span class="hljs-class">    <span class="hljs-keyword">extends</span> <span class="hljs-title">ProcessWindowFunction</span>&lt;<span class="hljs-title">Tuple2</span>&lt;<span class="hljs-title">String</span>, <span class="hljs-title">Long</span>&gt;, <span class="hljs-title">String</span>, <span class="hljs-title">String</span>, <span class="hljs-title">TimeWindow</span>&gt; </span>&#123;  <span class="hljs-meta">@Override</span>  <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">process</span><span class="hljs-params">(String key, Context context, Iterable&lt;Tuple2&lt;String, Long&gt;&gt; input, Collector&lt;String&gt; out)</span> </span>&#123;    <span class="hljs-keyword">long</span> count = <span class="hljs-number">0</span>;    <span class="hljs-keyword">for</span> (Tuple2&lt;String, Long&gt; in: input) &#123;      count++;    &#125;    out.collect(<span class="hljs-string">"Window: "</span> + context.window() + <span class="hljs-string">"count: "</span> + count);  &#125;&#125;</code></pre><p>该示例显示了一个ProcessWindowFunction，它对窗口中的元素进行计数。 另外，窗口功能将有关窗口的信息添加到输出中。</p><p>注意请注意，将ProcessWindowFunction用于简单的聚合（如count）效率很低。 下一节说明如何将ReduceFunction或AggregateFunction与ProcessWindowFunction结合使用，以获取增量聚合和ProcessWindowFunction的附加信息。</p><h2 id="ProcessWindowFunction-with-Incremental-Aggregation"><a href="#ProcessWindowFunction-with-Incremental-Aggregation" class="headerlink" title="ProcessWindowFunction with Incremental Aggregation"></a>ProcessWindowFunction with Incremental Aggregation</h2><p>可以将ProcessWindowFunction与ReduceFunction，AggregateFunction或FoldFunction组合以在元素到达窗口时对其进行增量聚合。 当窗口关闭时，将向ProcessWindowFunction提供聚合结果。 这使得它可以递增地计算窗口，同时可以访问ProcessWindowFunction的其他窗口元信息。</p><h4 id="使用ReduceFunction的增量窗口聚合"><a href="#使用ReduceFunction的增量窗口聚合" class="headerlink" title="使用ReduceFunction的增量窗口聚合"></a>使用ReduceFunction的增量窗口聚合</h4><p>以下示例显示了如何将增量ReduceFunction与ProcessWindowFunction结合使用以返回窗口中的最小事件以及该窗口的开始时间。</p><pre><code class="hljs java">DataStream&lt;SensorReading&gt; input = ...;input  .keyBy(&lt;key selector&gt;)  .timeWindow(&lt;duration&gt;)  .reduce(<span class="hljs-keyword">new</span> MyReduceFunction(), <span class="hljs-keyword">new</span> MyProcessWindowFunction());<span class="hljs-comment">// Function definitions</span><span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyReduceFunction</span> <span class="hljs-keyword">implements</span> <span class="hljs-title">ReduceFunction</span>&lt;<span class="hljs-title">SensorReading</span>&gt; </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">public</span> SensorReading <span class="hljs-title">reduce</span><span class="hljs-params">(SensorReading r1, SensorReading r2)</span> </span>&#123;      <span class="hljs-keyword">return</span> r1.value() &gt; r2.value() ? r2 : r1;  &#125;&#125;<span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyProcessWindowFunction</span></span><span class="hljs-class">    <span class="hljs-keyword">extends</span> <span class="hljs-title">ProcessWindowFunction</span>&lt;<span class="hljs-title">SensorReading</span>, <span class="hljs-title">Tuple2</span>&lt;<span class="hljs-title">Long</span>, <span class="hljs-title">SensorReading</span>&gt;, <span class="hljs-title">String</span>, <span class="hljs-title">TimeWindow</span>&gt; </span>&#123;  <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">process</span><span class="hljs-params">(String key,</span></span><span class="hljs-function"><span class="hljs-params">                    Context context,</span></span><span class="hljs-function"><span class="hljs-params">                    Iterable&lt;SensorReading&gt; minReadings,</span></span><span class="hljs-function"><span class="hljs-params">                    Collector&lt;Tuple2&lt;Long, SensorReading&gt;&gt; out)</span> </span>&#123;      SensorReading min = minReadings.iterator().next();      out.collect(<span class="hljs-keyword">new</span> Tuple2&lt;Long, SensorReading&gt;(context.window().getStart(), min));  &#125;&#125;</code></pre><h2 id="Using-per-window-state-in-ProcessWindowFunction"><a href="#Using-per-window-state-in-ProcessWindowFunction" class="headerlink" title="Using per-window state in ProcessWindowFunction"></a>Using per-window state in ProcessWindowFunction</h2><h1 id="Triggers"><a href="#Triggers" class="headerlink" title="Triggers"></a>Triggers</h1><h1 id="Evictors"><a href="#Evictors" class="headerlink" title="Evictors"></a>Evictors</h1><h1 id="Allowed-Lateness"><a href="#Allowed-Lateness" class="headerlink" title="Allowed Lateness"></a>Allowed Lateness</h1>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ConcurrentHashMap如何实现的</title>
      <link href="/2020/06/30/ConcurrentHashMap%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0/"/>
      <url>/2020/06/30/ConcurrentHashMap%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="Jdk1-7的ConcurrentHashMap"><a href="#Jdk1-7的ConcurrentHashMap" class="headerlink" title="Jdk1.7的ConcurrentHashMap"></a>Jdk1.7的ConcurrentHashMap</h1><p>jdk1.7中采用<code>Segment</code> + <code>HashEntry</code>的方式进行实现，结构如下：</p><p><img src="/images/jdk1.7_ConcurrentHashMap.png" srcset="/img/loading.gif" alt="concurrenthashmap_java8"></p><p>Segment数组的意义就是将一个大的table分割成多个小的table来进行加锁，也就是上面的提到的锁分离技术，而每一个Segment元素存储的是HashEntry数组+链表，这个和HashMap的数据存储结构一样</p><pre><code class="hljs java"><span class="hljs-keyword">int</span> sshift = <span class="hljs-number">0</span>;<span class="hljs-keyword">int</span> ssize = <span class="hljs-number">1</span>;<span class="hljs-keyword">while</span> (ssize &lt; concurrencyLevel) &#123;    ++sshift;    ssize &lt;&lt;= <span class="hljs-number">1</span>;&#125;</code></pre><p>如上所示，因为ssize用位于运算来计算（<code>ssize &lt;&lt;=1</code>），所以Segment的大小取值都是以2的N次方，无关concurrencyLevel的取值，当然concurrencyLevel最大只能用16位的二进制来表示，即65536，换句话说，Segment的大小最多65536个，没有指定concurrencyLevel元素初始化，Segment的大小ssize默认为16</p><p>每一个Segment元素下的HashEntry的初始化也是按照位于运算来计算，用cap来表示，如下所示</p><pre><code class="hljs java"><span class="hljs-keyword">int</span> cap = <span class="hljs-number">1</span>;<span class="hljs-keyword">while</span> (cap &lt; c)    cap &lt;&lt;= <span class="hljs-number">1</span>;</code></pre><p>如上所示，HashEntry大小的计算也是2的N次方（cap &lt;&lt;=1）， cap的初始值为1，所以HashEntry最小的容量为2</p><h2 id="put操作"><a href="#put操作" class="headerlink" title="put操作"></a>put操作</h2><p>对于ConcurrentHashMap的数据插入，这里要进行两次Hash去定位数据的存储位置</p><p>从上Segment的继承体系可以看出，Segment实现了ReentrantLock,也就带有锁的功能，当执行put操作时，会进行第一次key的hash来定位Segment的位置，如果该Segment还没有初始化，即通过CAS操作进行赋值，然后进行第二次hash操作，找到相应的HashEntry的位置，这里会利用继承过来的锁的特性，在将数据插入指定的HashEntry位置时（链表的尾端），会通过继承ReentrantLock的tryLock（）方法尝试去获取锁，如果获取成功就直接插入相应的位置，如果已经有线程获取该Segment的锁，那当前线程会以自旋的方式去继续的调用tryLock（）方法去获取锁，超过指定次数就挂起，等待唤醒</p><h1 id="Jdk1-8-ConcurrentHashMap"><a href="#Jdk1-8-ConcurrentHashMap" class="headerlink" title="Jdk1.8 ConcurrentHashMap"></a>Jdk1.8 ConcurrentHashMap</h1><p>Java 8为进一步提高并发性，摒弃了分段锁的方案，而是直接使用一个大的数组。同时为了提高哈希碰撞下的寻址性能，Java 8在链表长度超过一定阈值（8）时将链表（寻址时间复杂度为O(N)）转换为红黑树（寻址时间复杂度为O(long(N))）。其数据结构如下图所示</p><a id="more"></a><p><img src="/images/concurrenthashmap_java8.png" srcset="/img/loading.gif" alt="concurrenthashmap_java8"></p><h2 id="put操作-1"><a href="#put操作-1" class="headerlink" title="put操作"></a>put操作</h2><h2 id="get操作"><a href="#get操作" class="headerlink" title="get操作"></a>get操作</h2><pre><code class="hljs JAVA"> </code></pre>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink提交任务</title>
      <link href="/2020/06/26/Flink%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1/"/>
      <url>/2020/06/26/Flink%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1/</url>
      
        <content type="html"><![CDATA[<p>Flink以YarnCluster模式提交任务，且指定任务名和队列</p><p>flink run -m yarn-cluster -ynm PROD-fink-data-gather  –yarnqueue CClient /home/cclient/danke-flink-data-gather-prod.jar</p><a id="more"></a>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink中的并行度</title>
      <link href="/2020/06/25/Flink%E4%B8%AD%E7%9A%84%E5%B9%B6%E8%A1%8C%E5%BA%A6/"/>
      <url>/2020/06/25/Flink%E4%B8%AD%E7%9A%84%E5%B9%B6%E8%A1%8C%E5%BA%A6/</url>
      
        <content type="html"><![CDATA[<h1 id="Parallel-Execution（并行执行）"><a href="#Parallel-Execution（并行执行）" class="headerlink" title="Parallel Execution（并行执行）"></a>Parallel Execution（并行执行）</h1><p>一个任务被切分成几个并行实例执行，且每个并行实例处理输入任务的一部分数据，并行度会导致乱序问题，任务的并行实力数称为并行性</p><p>可以从三个层面限制并行度</p><h2 id="Execution-Environment-Level"><a href="#Execution-Environment-Level" class="headerlink" title="Execution Environment Level"></a>Execution Environment Level</h2><pre><code class="hljs java">env.setParallelism(<span class="hljs-number">3</span>);</code></pre><h2 id="Client-Level"><a href="#Client-Level" class="headerlink" title="Client Level"></a>Client Level</h2><pre><code class="hljs java">Client client = <span class="hljs-keyword">new</span> Client(jobManagerAddress, config, program.getUserCodeClassLoader());<span class="hljs-comment">// set the parallelism to 10 here</span>client.run(program, <span class="hljs-number">10</span>, <span class="hljs-keyword">true</span>);</code></pre><h2 id="System-Level"><a href="#System-Level" class="headerlink" title="System Level"></a>System Level</h2><p>在./conf/flink-conf.yaml中配置parallelism.default来设置并行度</p><h2 id="提示"><a href="#提示" class="headerlink" title="提示"></a>提示</h2><p>如果使用checkpoint的话需要设置一个最大并行度，避免从savepoint恢复时候导致性能问题</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink中的水印与时间</title>
      <link href="/2020/06/25/Flink%E6%97%B6%E9%97%B4/"/>
      <url>/2020/06/25/Flink%E6%97%B6%E9%97%B4/</url>
      
        <content type="html"><![CDATA[<h1 id="watermark-水印）"><a href="#watermark-水印）" class="headerlink" title="watermark(水印）"></a>watermark(水印）</h1><p>Flink中用于衡量event time进度的机制叫做水印</p><a id="more"></a><h2 id="官方定义"><a href="#官方定义" class="headerlink" title="官方定义"></a>官方定义</h2><p>watermark流(水印流)作为数据流的一部分，并带有一个时间戳t，当watermark t声明event time达到了t时刻，表名该数据流中不存在时间戳t`&lt;= t的元素</p><h2 id="对于watermark的理解"><a href="#对于watermark的理解" class="headerlink" title="对于watermark的理解"></a>对于watermark的理解</h2><p>在下文中的例子中，我们有一个带有时间戳的事件流，但是由于某种原因它们并不是按顺序到达的。图中的数字代表事件发生的时间戳。第一个到达的事件发生在时间 4，然后它后面跟着的是发生在更早时间（时间 2）的事件，以此类推：</p><p><img src="/images/warter_mark.png" srcset="/img/loading.gif" alt="warter_mark"></p><p>注意这是一个按照事件时间处理的例子，这意味着时间戳反映的是事件发生的时间，而不是处理事件的时间。事件时间（Event-Time）处理的强大之处在于，无论是在处理实时的数据还是重新处理历史的数据，基于事件时间创建的流计算应用都能保证结果是一样的。</p><p>现在假设我们正在尝试创建一个流计算排序算子。也就是处理一个乱序到达的事件流，并按照事件时间的顺序输出事件。</p><h3 id="理解1"><a href="#理解1" class="headerlink" title="理解1"></a>理解1</h3><p>数据流中的第一个元素的时间是 4，但是我们不能直接将它作为排序后数据流的第一个元素并输出它。因为数据是乱序到达的，也许有一个更早发生的数据还没有到达。事实上，我们能预见一些这个流的未来，也就是我们的排序算子至少要等到 2 这条数据的到达再输出结果。</p><p><em>有缓存，就必然有延迟。</em></p><h3 id="理解2"><a href="#理解2" class="headerlink" title="理解2"></a>理解2</h3><p>如果我们做错了，我们可能会永远等待下去。首先，我们的应用程序从看到时间 4 的数据，然后看到时间 2 的数据。是否会有一个比时间 2 更早的数据到达呢？也许会，也许不会。我们可以一直等下去，但可能永远看不到 1 。</p><p><em>最终，我们必须勇敢地输出 2 作为排序流的第一个结果。</em></p><h3 id="理解3"><a href="#理解3" class="headerlink" title="理解3"></a>理解3</h3><p>我们需要的是某种策略，它定义了对于任何带时间戳的事件流，何时停止等待更早数据的到来。</p><p><em>这正是 watermark 的作用，他们定义了何时不再等待更早的数据。</em></p><p>Flink 中的事件时间处理依赖于一种特殊的带时间戳的元素，成为 watermark，它们会由数据源或是 watermark 生成器插入数据流中。具有时间戳 t 的 watermark 可以被理解为断言了所有时间戳小于或等于 t 的事件都（在某种合理的概率上）已经到达了。</p><p>何时我们的排序算子应该停止等待，然后将事件 2 作为首个元素输出？答案是当收到时间戳为 2（或更大）的 watermark 时。</p><h3 id="理解4"><a href="#理解4" class="headerlink" title="理解4"></a>理解4</h3><p><em>我们可以设想不同的策略来生成 watermark。</em></p><p>我们知道每个事件都会延迟一段时间才到达，而这些延迟差异会比较大，所以有些事件会比其他事件延迟更多。一种简单的方法是假设这些延迟不会超过某个最大值。Flink 把这种策略称作 “有界无序生成策略”（bounded-out-of-orderness）。当然也有很多更复杂的方式去生成 watermark，但是对于大多数应用来说，固定延迟的方式已经足够了。</p><h1 id="Flink中的时间"><a href="#Flink中的时间" class="headerlink" title="Flink中的时间"></a>Flink中的时间</h1><h2 id="Event-Time"><a href="#Event-Time" class="headerlink" title="Event Time"></a>Event Time</h2><p>事件时间指事件在设备上的发生时间，该时间通常在消息进入Flink之前发生，且该事件发生的时间戳可以从每条记录单独提取</p><h2 id="Ingestion-Time-摄取时间"><a href="#Ingestion-Time-摄取时间" class="headerlink" title="Ingestion Time(摄取时间)"></a>Ingestion Time(摄取时间)</h2><p>摄取时间指的是消息进入flink的时间</p><h2 id="Processing-Time"><a href="#Processing-Time" class="headerlink" title="Processing Time"></a>Processing Time</h2><p>处理时间指的是执行某个任务时的系统时间，在消息进入flink且被window处理时发生</p><h2 id="三种时间关系示意图"><a href="#三种时间关系示意图" class="headerlink" title="三种时间关系示意图"></a>三种时间关系示意图</h2><p><img src="/images/times_clocks.svg" srcset="/img/loading.gif" alt="times_clocks"></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用arthas分析presto宕机原因</title>
      <link href="/2020/06/20/arthas/"/>
      <url>/2020/06/20/arthas/</url>
      
        <content type="html"><![CDATA[<h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>Presto半夜总是宕机</p><h1 id="解决思路"><a href="#解决思路" class="headerlink" title="解决思路"></a>解决思路</h1><p>使用arthas分析jvm使用情况</p><a id="more"></a><h2 id="下载arthas"><a href="#下载arthas" class="headerlink" title="下载arthas"></a>下载arthas</h2><p> curl -O <a href="https://alibaba.github.io/arthas/arthas-boot.jar" target="_blank" rel="noopener">https://alibaba.github.io/arthas/arthas-boot.jar</a> </p><h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><pre><code class="hljs mipsasm"><span class="hljs-keyword">java </span>-<span class="hljs-keyword">jar </span>arthas-<span class="hljs-keyword">boot.jar</span></code></pre><h2 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h2><h3 id="thread命令查看最占CPU的线程"><a href="#thread命令查看最占CPU的线程" class="headerlink" title="thread命令查看最占CPU的线程"></a>thread命令查看最占CPU的线程</h3><pre><code class="hljs excel">#展示当前最忙的前<span class="hljs-built_in">N</span>个线程并打印堆栈thread -<span class="hljs-built_in">n</span> <span class="hljs-number">3</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jvm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql迁移数据目录解决磁盘不足问题</title>
      <link href="/2020/06/18/mysql%E8%BF%81%E7%A7%BB%E6%95%B0%E6%8D%AE%E7%9B%AE%E5%BD%95%E8%A7%A3%E5%86%B3%E7%A3%81%E7%9B%98%E4%B8%8D%E8%B6%B3%E9%97%AE%E9%A2%98/"/>
      <url>/2020/06/18/mysql%E8%BF%81%E7%A7%BB%E6%95%B0%E6%8D%AE%E7%9B%AE%E5%BD%95%E8%A7%A3%E5%86%B3%E7%A3%81%E7%9B%98%E4%B8%8D%E8%B6%B3%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p>先关闭mysql数据库</p><p>mv /var/lib/mysql /data/mysql_data</p><a id="more"></a><p>创建目录</p><p>mkdir /data/mysql_data</p><p>chown mysql:mysql /data/mysql_data</p><p>做软连接</p><p>ln -s /data/mysql_data/mysql /var/lib/mysql</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode 160. 相交链表</title>
      <link href="/2020/06/17/%E6%89%BE%E5%88%B0%E4%B8%A4%E4%B8%AA%E5%8D%95%E9%93%BE%E8%A1%A8%E7%9B%B8%E4%BA%A4%E7%9A%84%E8%B5%B7%E5%A7%8B%E8%8A%82%E7%82%B9/"/>
      <url>/2020/06/17/%E6%89%BE%E5%88%B0%E4%B8%A4%E4%B8%AA%E5%8D%95%E9%93%BE%E8%A1%A8%E7%9B%B8%E4%BA%A4%E7%9A%84%E8%B5%B7%E5%A7%8B%E8%8A%82%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<p> 编写一个程序，找到两个单链表相交的起始节点。 </p><h3 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h3><p>快慢指针</p><p>因为两个链表长度可能不一样，但是相交后的元素都是一样的，可以使用快慢指针</p><p>***** 分别遍历链表，获取其长度，长链表命名为quickNode，短链表命名为slowNode，二者长度差为quickStep</p><p>***** quickNode先运行quickStep步之后，此时两个链表长度相等，用一个循环就能找到相同起始节点</p><a id="more"></a><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><pre><code class="hljs JAVA"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span> </span>&#123;   <span class="hljs-function"><span class="hljs-keyword">public</span> ListNode <span class="hljs-title">getIntersectionNode</span><span class="hljs-params">(ListNode headA, ListNode headB)</span> </span>&#123;        <span class="hljs-keyword">if</span>(headA == <span class="hljs-keyword">null</span> || headB == <span class="hljs-keyword">null</span>)&#123;            <span class="hljs-keyword">return</span> <span class="hljs-keyword">null</span>;        &#125;        <span class="hljs-keyword">int</span> aLength = getLength(headA);        <span class="hljs-keyword">int</span> bLength = getLength(headB);        ListNode quickNode = aLength &gt; bLength ? headA : headB;        ListNode slowNode = aLength &gt; bLength ? headB : headA;        <span class="hljs-keyword">int</span> quickStep = aLength &gt; bLength ? aLength - bLength : bLength - aLength;        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = quickStep; i &gt; <span class="hljs-number">0</span>; i--) &#123;            quickNode = quickNode.next;        &#125;        <span class="hljs-keyword">while</span> (quickNode != <span class="hljs-keyword">null</span>) &#123;            <span class="hljs-keyword">if</span> (quickNode == slowNode) &#123;                <span class="hljs-keyword">return</span> quickNode;            &#125;            quickNode = quickNode.next;            slowNode = slowNode.next;        &#125;        <span class="hljs-keyword">return</span> <span class="hljs-keyword">null</span>;    &#125;        <span class="hljs-function"><span class="hljs-keyword">private</span> <span class="hljs-keyword">int</span> <span class="hljs-title">getLength</span><span class="hljs-params">(ListNode head)</span> </span>&#123;        <span class="hljs-keyword">if</span>(head == <span class="hljs-keyword">null</span>)&#123;            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;        &#125;        <span class="hljs-keyword">int</span> count = <span class="hljs-number">0</span>;        ListNode aNode = head;        <span class="hljs-keyword">while</span> (aNode.next != <span class="hljs-keyword">null</span>) &#123;            aNode = aNode.next;            count++;        &#125;        <span class="hljs-keyword">return</span> count;    &#125;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>inode磁盘清理</title>
      <link href="/2020/06/15/inode%E7%A3%81%E7%9B%98%E6%B8%85%E7%90%86/"/>
      <url>/2020/06/15/inode%E7%A3%81%E7%9B%98%E6%B8%85%E7%90%86/</url>
      
        <content type="html"><![CDATA[<p>文章内容</p><a id="more"></a><h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>硬盘有剩余空间但一直提示no device space left，使用df -hl查看磁盘还有空间</p><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>使用df -i查看inode，发现占用率100%，说明碎片文件过多超过linux的文件数量的限制，使用如下命令查找碎片文件文件，并清理</p><pre><code class="hljs shell">find / -xdev -printf '%h\n' | sort | uniq -c | sort -k 1 -n</code></pre>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>建落地临时表写入很慢导致异常的解决方法</title>
      <link href="/2020/06/15/%E5%BB%BA%E8%90%BD%E5%9C%B0%E4%B8%B4%E6%97%B6%E8%A1%A8%E5%86%99%E5%85%A5%E5%BE%88%E6%85%A2%E5%AF%BC%E8%87%B4%E5%BC%82%E5%B8%B8%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/"/>
      <url>/2020/06/15/%E5%BB%BA%E8%90%BD%E5%9C%B0%E4%B8%B4%E6%97%B6%E8%A1%A8%E5%86%99%E5%85%A5%E5%BE%88%E6%85%A2%E5%AF%BC%E8%87%B4%E5%BC%82%E5%B8%B8%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>文章内容</p><a id="more"></a><p>建表时候指定为ORC格式</p><pre><code class="hljs sql"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> orc_table_with_null <span class="hljs-keyword">STORED</span> <span class="hljs-keyword">AS</span> ORC <span class="hljs-keyword">AS</span> <span class="hljs-keyword">SELECT</span> x,nullFROM <span class="hljs-keyword">empty</span>;</code></pre><p>Hive命令行调试模式</p><pre><code class="hljs routeros">hive --hiveconf hive.root.<span class="hljs-attribute">logger</span>=DEBUG,console</code></pre>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>beeline请求被提交到已经删除的hiveserver2节点</title>
      <link href="/2020/06/15/%E8%AF%B7%E6%B1%82%E8%A2%AB%E6%8F%90%E4%BA%A4%E5%88%B0%E5%B7%B2%E7%BB%8F%E5%88%A0%E9%99%A4%E7%9A%84hiveserver2/"/>
      <url>/2020/06/15/%E8%AF%B7%E6%B1%82%E8%A2%AB%E6%8F%90%E4%BA%A4%E5%88%B0%E5%B7%B2%E7%BB%8F%E5%88%A0%E9%99%A4%E7%9A%84hiveserver2/</url>
      
        <content type="html"><![CDATA[<p>文章内容</p><a id="more"></a><h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>在ambari集群上bigdata-client节点做hiveserver2自动拉起测试，增加了一个hiveserver2，然后再ambari删除该hiveserver2</p><p>然后出现请求被提交到已经删除的hiveserver2节点</p><p><img src="/images/image-20200614163951660.png" srcset="/img/loading.gif" alt="image-20200614163951660"></p><p>在zookeeper节点上执行zookeeper-cli.sh进入zookeeper命令行</p><p>执行删除无效的节点命令</p><pre><code class="hljs shell">delete /hiveserver2/serverUri=bigdata-client:10000;version=1.2.1000.2.6.5.1175-1;sequence=0000000010</code></pre><p>过一段时间还是会出现上述错误，之后发现自动拉起服务没删除……</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>leetcode 2.两数相加</title>
      <link href="/2020/06/14/%E7%AE%97%E6%B3%95-%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0/"/>
      <url>/2020/06/14/%E7%AE%97%E6%B3%95-%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="2-两数相加"><a href="#2-两数相加" class="headerlink" title="2. 两数相加"></a><a href="https://leetcode-cn.com/problems/add-two-numbers/" target="_blank" rel="noopener">2. 两数相加</a></h1><p>给出两个 <strong>非空</strong> 的链表用来表示两个非负的整数。其中，它们各自的位数是按照 <strong>逆序</strong> 的方式存储的，并且它们的每个节点只能存储 <strong>一位</strong> 数字。</p><p>如果，我们将这两个数相加起来，则会返回一个新的链表来表示它们的和。</p><p>您可以假设除了数字 0 之外，这两个数都不会以 0 开头。</p><p><strong>示例：</strong></p><pre><code class="hljs angelscript">输入：(<span class="hljs-number">2</span> -&gt; <span class="hljs-number">4</span> -&gt; <span class="hljs-number">3</span>) + (<span class="hljs-number">5</span> -&gt; <span class="hljs-number">6</span> -&gt; <span class="hljs-number">4</span>)输出：<span class="hljs-number">7</span> -&gt; <span class="hljs-number">0</span> -&gt; <span class="hljs-number">8</span>原因：<span class="hljs-number">342</span> + <span class="hljs-number">465</span> = <span class="hljs-number">807</span></code></pre><a id="more"></a><p>链表ListNode结构</p><pre><code class="hljs JAVA"><span class="hljs-keyword">package</span> com.bd.leetcode.leetcode_2;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ListNode</span> </span>&#123;    <span class="hljs-keyword">int</span> val;    ListNode next;    ListNode(<span class="hljs-keyword">int</span> x) &#123;        val = x;    &#125;    ListNode(<span class="hljs-keyword">int</span> val, ListNode next) &#123;        <span class="hljs-keyword">this</span>.val = val;        <span class="hljs-keyword">this</span>.next = next;    &#125;&#125;</code></pre><h1 id="第一次实现代码"><a href="#第一次实现代码" class="headerlink" title="第一次实现代码"></a>第一次实现代码</h1><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.bd.leetcode.leetcode_2;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span> </span>&#123;    <span class="hljs-keyword">private</span> ListNode first;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> <span class="hljs-title">getLength</span><span class="hljs-params">(ListNode l1)</span> </span>&#123;        <span class="hljs-keyword">int</span> count = <span class="hljs-number">0</span>;        <span class="hljs-keyword">while</span> (l1 != <span class="hljs-keyword">null</span>) &#123;            l1 = l1.next;            count++;        &#125;        <span class="hljs-keyword">return</span> count;    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> ListNode <span class="hljs-title">arr2ListNode</span><span class="hljs-params">(<span class="hljs-keyword">int</span>[] arr)</span> </span>&#123;        ListNode firstNode = <span class="hljs-keyword">null</span>;        <span class="hljs-keyword">if</span> (arr == <span class="hljs-keyword">null</span> || arr.length == <span class="hljs-number">0</span>) &#123;            <span class="hljs-keyword">return</span> <span class="hljs-keyword">null</span>;        &#125;        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; arr.length; i++) &#123;            <span class="hljs-keyword">if</span> (firstNode == <span class="hljs-keyword">null</span>) &#123;                firstNode = <span class="hljs-keyword">new</span> ListNode(arr[i]);            &#125; <span class="hljs-keyword">else</span> &#123;                ListNode oldNode = firstNode;                firstNode = <span class="hljs-keyword">new</span> ListNode(arr[i]);                firstNode.next = oldNode;<span class="hljs-comment">//                ListNode oldNode = firstNode;</span><span class="hljs-comment">//                firstNode = new ListNode(arr[i],oldNode);</span>            &#125;        &#125;        <span class="hljs-keyword">return</span> firstNode;    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> ListNode <span class="hljs-title">reverse</span><span class="hljs-params">(ListNode a)</span> </span>&#123;        ListNode pre = <span class="hljs-keyword">null</span>;        ListNode cur = a;        ListNode tmp;        <span class="hljs-keyword">while</span> (<span class="hljs-keyword">null</span> != cur) &#123;            tmp = cur.next; <span class="hljs-comment">// 保存当前节点的下一个节点</span>            cur.next = pre; <span class="hljs-comment">// 把当前节点的下一个节点重置为上一个节点</span>            pre = cur; <span class="hljs-comment">//下次遍历时候需要使用的上一个节点</span>            cur = tmp; <span class="hljs-comment">//下次遍历时候需要的当前节点</span>        &#125;        <span class="hljs-keyword">return</span> pre;    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> ListNode <span class="hljs-title">addTwoNumbers</span><span class="hljs-params">(ListNode l1, ListNode l2)</span> </span>&#123;        <span class="hljs-keyword">int</span> l1Len = getLength(l1);        <span class="hljs-keyword">int</span> l2Len = getLength(l2);        <span class="hljs-keyword">int</span> len = l1Len &gt;= l2Len ? l1Len : l2Len;        <span class="hljs-keyword">int</span> flag = <span class="hljs-number">0</span>;        <span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>;        <span class="hljs-keyword">do</span> &#123;            <span class="hljs-keyword">int</span> l1Value = l1 != <span class="hljs-keyword">null</span> ? l1.val : <span class="hljs-number">0</span>;            <span class="hljs-keyword">int</span> l2Value = l2 != <span class="hljs-keyword">null</span> ? l2.val : <span class="hljs-number">0</span>;            <span class="hljs-keyword">int</span> currVal = <span class="hljs-number">0</span>;            <span class="hljs-keyword">if</span> (l1Value + l2Value + flag &gt; <span class="hljs-number">9</span>) &#123;                currVal = l1Value + l2Value + flag - <span class="hljs-number">10</span>;                flag = <span class="hljs-number">1</span>;            &#125; <span class="hljs-keyword">else</span> &#123;                currVal = l1Value + l2Value + flag;                flag = <span class="hljs-number">0</span>;            &#125;            <span class="hljs-keyword">if</span> (first == <span class="hljs-keyword">null</span>) &#123;                first = <span class="hljs-keyword">new</span> ListNode(currVal);            &#125; <span class="hljs-keyword">else</span> &#123;                ListNode oldFirst = first;                first = <span class="hljs-keyword">new</span> ListNode(currVal);                first.next = oldFirst;            &#125;            l1 = l1 != <span class="hljs-keyword">null</span> ? l1.next : <span class="hljs-keyword">null</span>;            l2 = l2 != <span class="hljs-keyword">null</span> ? l2.next : <span class="hljs-keyword">null</span>;            i++;        &#125; <span class="hljs-keyword">while</span> (i &lt; len || flag &gt; <span class="hljs-number">0</span>);<span class="hljs-comment">//flag&gt;0说明有进位需要再次执行一边</span>        <span class="hljs-keyword">return</span> reverse(first);    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">walk</span><span class="hljs-params">(ListNode a)</span> </span>&#123;        StringBuilder s = <span class="hljs-keyword">new</span> StringBuilder();        <span class="hljs-keyword">while</span> (a != <span class="hljs-keyword">null</span>) &#123;            s.append(a.val).append(<span class="hljs-string">" "</span>);            a = a.next;        &#125;        System.out.println(s.toString());    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        Solution s = <span class="hljs-keyword">new</span> Solution();        ListNode a = s.arr2ListNode(<span class="hljs-keyword">new</span> <span class="hljs-keyword">int</span>[]&#123;<span class="hljs-number">1</span>&#125;);        ListNode b = s.arr2ListNode(<span class="hljs-keyword">new</span> <span class="hljs-keyword">int</span>[]&#123;<span class="hljs-number">9</span>, <span class="hljs-number">9</span>&#125;);        s.walk(a);        s.walk(b);        ListNode c = s.addTwoNumbers(a, b);        s.walk(c);    &#125;&#125;</code></pre><h1 id="优化后代码"><a href="#优化后代码" class="headerlink" title="优化后代码"></a>优化后代码</h1><pre><code class="hljs java"><span class="hljs-function"><span class="hljs-keyword">public</span> ListNode <span class="hljs-title">addTwoNumbers</span><span class="hljs-params">(ListNode l1, ListNode l2)</span> </span>&#123;        <span class="hljs-keyword">int</span> flag = <span class="hljs-number">0</span>;        ListNode root = <span class="hljs-keyword">new</span> ListNode(<span class="hljs-number">0</span>); <span class="hljs-comment">//记录假head节点位置，真head为root.next</span>        ListNode cursor = root;         <span class="hljs-keyword">while</span> (l1 != <span class="hljs-keyword">null</span> || l2 != <span class="hljs-keyword">null</span> || flag &gt; <span class="hljs-number">0</span>)&#123;            <span class="hljs-keyword">int</span> l1Value = l1 != <span class="hljs-keyword">null</span> ? l1.val : <span class="hljs-number">0</span>;            <span class="hljs-keyword">int</span> l2Value = l2 != <span class="hljs-keyword">null</span> ? l2.val : <span class="hljs-number">0</span>;            <span class="hljs-keyword">int</span> currVal = <span class="hljs-number">0</span>;            <span class="hljs-keyword">if</span> (l1Value + l2Value + flag &gt; <span class="hljs-number">9</span>) &#123;                currVal = l1Value + l2Value + flag - <span class="hljs-number">10</span>;                flag = <span class="hljs-number">1</span>;            &#125; <span class="hljs-keyword">else</span> &#123;                currVal = l1Value + l2Value + flag;                flag = <span class="hljs-number">0</span>;            &#125;            cursor.next = <span class="hljs-keyword">new</span> ListNode(currVal);            cursor = cursor.next;  <span class="hljs-comment">// 指针后移</span>            l1 = l1 != <span class="hljs-keyword">null</span> ? l1.next : <span class="hljs-keyword">null</span>;            l2 = l2 != <span class="hljs-keyword">null</span> ? l2.next : <span class="hljs-keyword">null</span>;        &#125;        <span class="hljs-keyword">return</span> root.next;    &#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二叉树</title>
      <link href="/2020/06/14/%E7%AE%97%E6%B3%95-%E4%BA%8C%E5%8F%89%E6%A0%91/"/>
      <url>/2020/06/14/%E7%AE%97%E6%B3%95-%E4%BA%8C%E5%8F%89%E6%A0%91/</url>
      
        <content type="html"><![CDATA[<p>二叉树需要声明一个Node节点，包含节点得值，以及左右两个子节点</p><p>Node节点代码如下</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.code.note.tree;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Node</span>&lt;<span class="title">T</span> <span class="keyword">extends</span> <span class="title">Comparable</span>&gt;</span>&#123;</span><br><span class="line">    T value;</span><br><span class="line">    Node left;</span><br><span class="line">    Node right;</span><br><span class="line"></span><br><span class="line">    Node(T value) &#123;</span><br><span class="line">        <span class="keyword">this</span>.value = value;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><a id="more"></a><p>BinaryTree代码如下</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.code.note.tree;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BinaryTree</span>&lt;<span class="title">T</span> <span class="keyword">extends</span> <span class="title">Comparable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Node&lt;T&gt; root;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">put</span><span class="params">(T value)</span> </span>&#123;</span><br><span class="line">        root = put(root, value);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> Node <span class="title">put</span><span class="params">(Node&lt;T&gt; x, T value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (x == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> Node(value);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> cmp = value.compareTo(x.value);</span><br><span class="line">        <span class="keyword">if</span> (cmp &lt; <span class="number">0</span>) &#123;</span><br><span class="line">            x.left = put(x.left, value);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (cmp &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            x.right = put(x.right, value);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> x;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">traverse</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        traverseRecursive(root);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">traverseRecursive</span><span class="params">(Node node)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 前序遍历</span></span><br><span class="line">        <span class="keyword">if</span> (node == <span class="keyword">null</span>) <span class="keyword">return</span>;</span><br><span class="line">        System.out.println(node.value);</span><br><span class="line">        traverseRecursive(node.left);</span><br><span class="line">        <span class="comment">// 中序遍历</span></span><br><span class="line">        traverseRecursive(node.right);</span><br><span class="line">        <span class="comment">// 后序遍历</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>测试用例</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.code.note.tree;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BinaryTreeTest</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line"></span><br><span class="line">        BinaryTree&lt;Integer&gt; binaryTree = <span class="keyword">new</span> BinaryTree&lt;Integer&gt;();</span><br><span class="line">        binaryTree.put(<span class="number">3</span>);</span><br><span class="line">        binaryTree.put(<span class="number">5</span>);</span><br><span class="line">        binaryTree.put(<span class="number">1</span>);</span><br><span class="line">        binaryTree.put(<span class="number">10</span>);</span><br><span class="line"></span><br><span class="line">        binaryTree.traverse();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ambari集群安装</title>
      <link href="/2020/06/03/Ambari%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/"/>
      <url>/2020/06/03/Ambari%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<p>文章内容</p><a id="more"></a><h2 id="目前的机器"><a href="#目前的机器" class="headerlink" title="目前的机器"></a>目前的机器</h2><p>172.22.222.109</p><h2 id="安装ambari-server"><a href="#安装ambari-server" class="headerlink" title="安装ambari-server"></a>安装ambari-server</h2><p>在/etc/yum.repos.d/ambari.repo中新增ambari源</p><pre><code class="hljs angelscript">cat /etc/yum.repos.d/ambari.repo#VERSION_NUMBER=<span class="hljs-number">2.6</span><span class="hljs-number">.2</span><span class="hljs-number">.2</span><span class="hljs-number">-1</span>[ambari<span class="hljs-number">-2.6</span><span class="hljs-number">.2</span><span class="hljs-number">.2</span>]name=ambari Version - ambari<span class="hljs-number">-2.6</span><span class="hljs-number">.2</span><span class="hljs-number">.2</span>baseurl=http:<span class="hljs-comment">//public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.6.2.2</span>gpgcheck=<span class="hljs-number">1</span>gpgkey=http:<span class="hljs-comment">//public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.6.2.2/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins</span>enabled=<span class="hljs-number">1</span>priority=<span class="hljs-number">1</span></code></pre><p>执行安装命令</p><p>yum install install ambari-server</p><h2 id="安装MariaDB-10-1"><a href="#安装MariaDB-10-1" class="headerlink" title="安装MariaDB 10.1"></a>安装MariaDB 10.1</h2><pre><code class="hljs armasm"><span class="hljs-symbol">cat</span> /etc/yum.repos.d/<span class="hljs-keyword">Mariadb.repo</span><span class="hljs-keyword">[mariadb]</span><span class="hljs-keyword">name </span>= <span class="hljs-keyword">MariaDB</span><span class="hljs-keyword">baseurl </span>= https://mirrors.tuna.tsinghua.edu.cn/<span class="hljs-keyword">mariadb/yum/10.1/centos7-amd64</span><span class="hljs-keyword">gpgkey </span>= https://mirrors.tuna.tsinghua.edu.cn/<span class="hljs-keyword">mariadb/yum//RPM-GPG-KEY-MariaDB</span><span class="hljs-keyword">gpgcheck=1</span><span class="hljs-keyword"></span><span class="hljs-keyword">yum </span>install <span class="hljs-keyword">mariadb-server </span><span class="hljs-keyword">mariadb </span><span class="hljs-keyword">mariadb-client </span><span class="hljs-keyword">mariadb-devel</span></code></pre><h2 id="部署的目录"><a href="#部署的目录" class="headerlink" title="部署的目录"></a>部署的目录</h2><p>/var/lib/ambari-server</p><h2 id="web访问地址"><a href="#web访问地址" class="headerlink" title="web访问地址"></a>web访问地址</h2><p><a href="http://172.22.222.109:9090/" target="_blank" rel="noopener">http://172.22.222.109:9090</a></p><h2 id="配置文件的位置"><a href="#配置文件的位置" class="headerlink" title="配置文件的位置"></a>配置文件的位置</h2><p>/etc/ambari-server/conf</p><h2 id="管理脚本"><a href="#管理脚本" class="headerlink" title="管理脚本"></a>管理脚本</h2><p>service ambari-server status</p><p>service ambari-server restart</p><p>service ambari-server stop</p><p>service ambari-server start</p><h2 id="添加Client机器报SSL错误问题"><a href="#添加Client机器报SSL错误问题" class="headerlink" title="添加Client机器报SSL错误问题"></a>添加Client机器报SSL错误问题</h2><p>编辑要增加的客户端机器/etc/ambari-agent/conf/ambari-agent.ini</p><p>在security标签增加</p><p>force_https_protocol=PROTOCOL_TLSv1_2</p><h2 id="安装时报删除服务失败问题"><a href="#安装时报删除服务失败问题" class="headerlink" title="安装时报删除服务失败问题"></a>安装时报删除服务失败问题</h2><p>查看/var/log/ambari-server/ambari-server.log日志报</p><p><em>Caught AmbariException when modifying a resource</em><br><em>org.apache.ambari.server.AmbariException: Could not delete cluster, clusterName=localvps</em></p><p><em>01 Apr 2020 12:26:06,836 WARN [ambari-client-thread-34] ServiceComponentImpl:492 - Found non removable hostcomponent when trying to delete service component, clusterName=localvps, serviceName=HIVE, componentName=MYSQL_SERVER, state=STARTED, hostname=localvps</em><br><em>01 Apr 2020 12:26:06,836 WARN [ambari-client-thread-34] ServiceImpl:509 - Found non removable component when trying to delete service, clusterName=localvps, serviceName=HIVE, componentName=MYSQL_SERVER</em></p><p>这个问题是由于安装hive时候选择New Database，之后又选择exist Database导致的，删除mysql数据库后重新安装MariaDB 10.1之后该问题消失</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker安装superset</title>
      <link href="/2020/06/03/Docker%E5%AE%89%E8%A3%85superset/"/>
      <url>/2020/06/03/Docker%E5%AE%89%E8%A3%85superset/</url>
      
        <content type="html"><![CDATA[<p>文章内容</p><a id="more"></a><h1 id="挂载目录到本地"><a href="#挂载目录到本地" class="headerlink" title="挂载目录到本地"></a>挂载目录到本地</h1><p>mkdir /Users/yangaipeng/Desktop/incubator-superset/docker_superset/superset</p><h1 id="启动docker-superset"><a href="#启动docker-superset" class="headerlink" title="启动docker superset"></a>启动docker superset</h1><p>docker run -d -p 8088:8088 -v /Users/yangaipeng/Desktop/incubator-superset/docker_superset/superset:/home/superset –name superset2 superset-release-tag</p><h1 id="增加配置文件"><a href="#增加配置文件" class="headerlink" title="增加配置文件"></a>增加配置文件</h1><p>将配置文件superset_config.py放到/Users/yangaipeng/Desktop/incubator-superset/docker_superset/superset目录下</p><p>在superset_config.py将mysql地址配置为host.docker.internal，表示让docker中的superset访问本机数据库</p><h1 id="设置用户名和密码"><a href="#设置用户名和密码" class="headerlink" title="设置用户名和密码"></a>设置用户名和密码</h1><p>docker exec -it 2326fa0a7d8b fabmanager create-admin –app superset</p><h1 id="初始化数据库"><a href="#初始化数据库" class="headerlink" title="初始化数据库"></a>初始化数据库</h1><p>docker exec -it 2326fa0a7d8b superset db upgrade</p><h1 id="superset初始化"><a href="#superset初始化" class="headerlink" title="superset初始化"></a>superset初始化</h1><p>docker exec -it 2326fa0a7d8b superset init</p><h1 id="开启superset服务"><a href="#开启superset服务" class="headerlink" title="开启superset服务"></a>开启superset服务</h1><p>docker exec -it 2326fa0a7d8b superset runserver</p><h1 id="使用root用户运行docker-bash"><a href="#使用root用户运行docker-bash" class="headerlink" title="使用root用户运行docker bash"></a>使用root用户运行docker bash</h1><p>docker –user root -it exec 1b9e081ef94f bash</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hue配置Presto</title>
      <link href="/2020/06/03/Hue%E9%85%8D%E7%BD%AEPresto/"/>
      <url>/2020/06/03/Hue%E9%85%8D%E7%BD%AEPresto/</url>
      
        <content type="html"><![CDATA[<p>文章内容</p><a id="more"></a><h1 id="python3安装pyhive"><a href="#python3安装pyhive" class="headerlink" title="python3安装pyhive"></a>python3安装pyhive</h1><p>进入hue安装目录/usr/local/hue/hue-4.3.0/</p><p>./build/env/bin/pip install pyhive</p><p>在/usr/local/hue/hue-4.3.0/desktop目录编辑</p><pre><code class="hljs lua">vim desktop/conf/hue.ini<span class="hljs-string">[[[presto]]</span>]      name = Presto      interface=sqlalchemy      options=<span class="hljs-string">'&#123;"url": "presto://172.22.222.89:9090/hive_danke/default"&#125;'</span></code></pre><h1 id="重启hue"><a href="#重启hue" class="headerlink" title="重启hue"></a>重启hue</h1><p>kill pid </p><p>/usr/local/hue/hue-4.3.0/build/env/bin/python2.7 /usr/local/hue/hue-4.3.0/build/env/bin/hue runcherrypyserver</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM垃圾回收算法与收集器</title>
      <link href="/2020/06/03/JVM%E7%AE%97%E6%B3%95/"/>
      <url>/2020/06/03/JVM%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>文章内容</p><a id="more"></a><h1 id="垃圾回收方法"><a href="#垃圾回收方法" class="headerlink" title="垃圾回收方法"></a>垃圾回收方法</h1><h2 id="标记清除算法"><a href="#标记清除算法" class="headerlink" title="标记清除算法"></a>标记清除算法</h2><p>适用：年轻带，年老带</p><p>缺点：不连续内存碎片，导致大对象直接进入年老带或者直接触发下一次GC</p><h2 id="复制算法"><a href="#复制算法" class="headerlink" title="复制算法"></a>复制算法</h2><p>适用：年轻带（更适合“朝生夕死”），不适用于年老带，因为可能存在100%存活的极端情况</p><p>缺点：复制算法会导致浪费一定的survivor内存空间，现在内存算法都是经过改良更适合年轻带，且堆中的Eden和2个survivor比例是 8：1：1</p><h2 id="标记整理"><a href="#标记整理" class="headerlink" title="标记整理"></a>标记整理</h2><p>适用：年老带（更适合“朝生夕死”）</p><h2 id="分带收集算法"><a href="#分带收集算法" class="headerlink" title="分带收集算法"></a>分带收集算法</h2><p>适用：根据各个年代的特点使用最适当的收集算法</p><h1 id="部分垃圾回收器"><a href="#部分垃圾回收器" class="headerlink" title="部分垃圾回收器"></a>部分垃圾回收器</h1><h2 id="CMS收集器"><a href="#CMS收集器" class="headerlink" title="CMS收集器"></a>CMS收集器</h2><p>初始标记（此处会stop-the-world）</p><p>并发标记</p><p>重新标记（此处会stop-the-world）</p><p>并发清除</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java线程池</title>
      <link href="/2020/06/03/Java%E7%BA%BF%E7%A8%8B%E6%B1%A0/"/>
      <url>/2020/06/03/Java%E7%BA%BF%E7%A8%8B%E6%B1%A0/</url>
      
        <content type="html"><![CDATA[<p>文章内容</p><a id="more"></a><h1 id="TheadPoolExecutor"><a href="#TheadPoolExecutor" class="headerlink" title="TheadPoolExecutor"></a>TheadPoolExecutor</h1><h2 id="队列满时候的丢弃策略"><a href="#队列满时候的丢弃策略" class="headerlink" title="队列满时候的丢弃策略"></a>队列满时候的丢弃策略</h2><p>DiscardPolicy：不处理，直接丢弃掉；<br>AbortPolicy：直接抛出 RejectedExecutionException 异常，这是默认的拒绝策略；<br>DiscardOldestPolicy：丢弃最老的任务，并执行当前任务；<br>CallerRunsPolicy：由调用线程本身运行任务，以减缓提交速度，会阻塞主线程；</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux挂载分区parted命令</title>
      <link href="/2020/06/03/Linux%E6%8C%82%E8%BD%BD%E5%88%86%E5%8C%BAparted%E5%91%BD%E4%BB%A4/"/>
      <url>/2020/06/03/Linux%E6%8C%82%E8%BD%BD%E5%88%86%E5%8C%BAparted%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<p>文章内容</p><a id="more"></a><p>分区命令如下</p><pre><code class="hljs shell">mkdir /data1sleep 1mkdir /data2sleep 1mkdir /data3sleep 1mkdir /data4sleep 1mkdir /data5sleep 1mkdir /data6sleep 1mkdir /data7sleep 1mkdir /data8sleep 1\#execute b 1parted /dev/vdb mklabel gpt mkpart primary 0% 100%mkfs.xfs /dev/vdb1mount /dev/vdb1 /data1uuid=`blkid |grep /dev/vdb|awk '&#123;print $2&#125;' | xargs echo`echo $uuid" /data1 xfs defaults 1 1"&gt;&gt;/etc/fstab\#execute c 2parted /dev/vdc mklabel gpt mkpart primary 0% 100%sleep 3mkfs.xfs /dev/vdc1sleep 3mount /dev/vdc1 /data2sleep 3uuid=`blkid |grep /dev/vdc|awk '&#123;print $2&#125;' | xargs echo`echo $uuid" /data2 xfs defaults 1 1"&gt;&gt;/etc/fstab\#execute d 3parted /dev/vdd mklabel gpt mkpart primary 0% 100%sleep 3mkfs.xfs /dev/vdd1sleep 3mount /dev/vdd1 /data3sleep 3uuid=`blkid |grep /dev/vdd|awk '&#123;print $2&#125;' | xargs echo`echo $uuid" /data3 xfs defaults 1 1"&gt;&gt;/etc/fstab\#execute e 4parted /dev/vde mklabel gpt mkpart primary 0% 100%sleep 3mkfs.xfs /dev/vde1sleep 3mount /dev/vde1 /data4sleep 3uuid=`blkid |grep /dev/vde|awk '&#123;print $2&#125;' | xargs echo`echo $uuid" /data4 xfs defaults 1 1"&gt;&gt;/etc/fstab\#execute f 5parted /dev/vdf mklabel gpt mkpart primary 0% 100%sleep 3mkfs.xfs /dev/vdf1sleep 3mount /dev/vdf1 /data5sleep 3uuid=`blkid |grep /dev/vde|awk '&#123;print $2&#125;' | xargs echo`echo $uuid" /data5 xfs defaults 1 1"&gt;&gt;/etc/fstab\#execute g 6parted /dev/vdg mklabel gpt mkpart primary 0% 100%sleep 3mkfs.xfs /dev/vdg1sleep 3mount /dev/vdg1 /data6sleep 3uuid=`blkid |grep /dev/vdg|awk '&#123;print $2&#125;' | xargs echo`echo $uuid" /data6 xfs defaults 1 1"&gt;&gt;/etc/fstab\#execute h 7parted /dev/vdh mklabel gpt mkpart primary 0% 100%sleep 3mkfs.xfs /dev/vdh1sleep 3mount /dev/vdh1 /data7sleep 3uuid=`blkid |grep /dev/vdh|awk '&#123;print $2&#125;' | xargs echo`echo $uuid" /data7 xfs defaults 1 1"&gt;&gt;/etc/fstab\#execute i 8parted /dev/vdi mklabel gpt mkpart primary 0% 100%sleep 3mkfs.xfs /dev/vdi1sleep 3mount /dev/vdi1 /data8sleep 3uuid=`blkid |grep /dev/vdi|awk '&#123;print $2&#125;' | xargs echo`echo $uuid" /data8 xfs defaults 1 1"&gt;&gt;/etc/fstab\#execute j 9parted /dev/vdj mklabel gpt mkpart primary 0% 100%sleep 3mkfs.xfs /dev/vdijsleep 3mount /dev/vdij /datasleep 3uuid=`blkid |grep /dev/vdj|awk '&#123;print $2&#125;' | xargs echo`echo $uuid" /data xfs defaults 1 1"&gt;&gt;/etc/fstab</code></pre>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Presto导出到csv</title>
      <link href="/2020/06/03/Presto%E5%AF%BC%E5%87%BA%E5%88%B0csv/"/>
      <url>/2020/06/03/Presto%E5%AF%BC%E5%87%BA%E5%88%B0csv/</url>
      
        <content type="html"><![CDATA[<p>文章内容</p><a id="more"></a><p>nohup presto –server localhost:9090 –catalog hive_danke –schema default -f /tmp/tmp.sql –output-format CSV &amp;&gt; /data/xiongyan_yanzheng.csv &amp;</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark-submit时候添加jars</title>
      <link href="/2020/06/03/spark-submit%E6%97%B6%E5%80%99%E6%B7%BB%E5%8A%A0jars/"/>
      <url>/2020/06/03/spark-submit%E6%97%B6%E5%80%99%E6%B7%BB%E5%8A%A0jars/</url>
      
        <content type="html"><![CDATA[<p>文章内容</p><a id="more"></a><h1 id="spark-submit时单独添加jars"><a href="#spark-submit时单独添加jars" class="headerlink" title="spark-submit时单独添加jars"></a>spark-submit时单独添加jars</h1><p>spark-submit –jars app-util-1.0.0.jar –class com.dankegongyu.dailyremain.DailyRemain –master yarn –deploy-mode client /data/bigdata/app-daily-remain-1.0.0.jar $yesterday</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用Macpro创建centos7启动盘</title>
      <link href="/2020/06/03/%E4%BD%BF%E7%94%A8Macpro%E5%88%9B%E5%BB%BAcentos7%E5%90%AF%E5%8A%A8%E7%9B%98/"/>
      <url>/2020/06/03/%E4%BD%BF%E7%94%A8Macpro%E5%88%9B%E5%BB%BAcentos7%E5%90%AF%E5%8A%A8%E7%9B%98/</url>
      
        <content type="html"><![CDATA[<p>文章内容</p><a id="more"></a><h1 id="Macpro-查看新增优盘"><a href="#Macpro-查看新增优盘" class="headerlink" title="Macpro 查看新增优盘"></a>Macpro 查看新增优盘</h1><p>diskutil list</p><p>umount u盘</p><p>diskutil unmountDisk /dev/disk2</p><h1 id="Macpro-dd命令写入"><a href="#Macpro-dd命令写入" class="headerlink" title="Macpro dd命令写入"></a>Macpro dd命令写入</h1><p>sudo dd if=/Users/yxs1112003/Downloads/CentOS-7-x86_64-DVD-1503-01.iso of=/dev/disk2 bs=4m</p><p>centos7 显示进度条的dd命令</p><p>dd if=./centos7.1.iso of=/dev/sdb1 status=progress</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>手动编译安装flink，适配ambari hdp版本hadoop</title>
      <link href="/2020/06/03/%E6%89%8B%E5%8A%A8%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85flink%EF%BC%8C%E9%80%82%E9%85%8Dhdp%E7%89%88%E6%9C%AChadoop/"/>
      <url>/2020/06/03/%E6%89%8B%E5%8A%A8%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85flink%EF%BC%8C%E9%80%82%E9%85%8Dhdp%E7%89%88%E6%9C%AChadoop/</url>
      
        <content type="html"><![CDATA[<p>文章内容</p><a id="more"></a><h1 id="找到ambari-hadoop版本"><a href="#找到ambari-hadoop版本" class="headerlink" title="找到ambari hadoop版本"></a>找到ambari hadoop版本</h1><p>在hdp下载目录找到当前hadoop版本</p><p><a href="http://repo.hortonworks.com/content/repositories/releases/org/apache/hadoop/hadoop-common/" target="_blank" rel="noopener">http://repo.hortonworks.com/content/repositories/releases/org/apache/hadoop/hadoop-common/</a></p><p>前面是hadoop版本，后面是hdp版本</p><h1 id="clone-flink"><a href="#clone-flink" class="headerlink" title="clone flink"></a>clone flink</h1><p>clone flink源码</p><p>git clone <a href="https://github.com/apache/flink.git" target="_blank" rel="noopener">https://github.com/apache/flink.git</a></p><p>切换分支</p><h1 id="手动编译"><a href="#手动编译" class="headerlink" title="手动编译"></a>手动编译</h1><p>在工程目录执行 git checkout release-1.6.4-rc1</p><p>Flink编译安装</p><p>mvn clean install -DskipTests -Drat.skip=true -Pvendor-repos -Dhadoop.version=2.7.3.2.6.5.0-292</p><p>参考：</p><p><a href="https://www.bbsmax.com/A/B0zqV4OnJv/" target="_blank" rel="noopener">https://www.bbsmax.com/A/B0zqV4OnJv/</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ambari集群datanode出现OOM问题</title>
      <link href="/2012/06/15/ambari%E9%9B%86%E7%BE%A4datanode%E5%87%BA%E7%8E%B0OOM%E9%97%AE%E9%A2%98/"/>
      <url>/2012/06/15/ambari%E9%9B%86%E7%BE%A4datanode%E5%87%BA%E7%8E%B0OOM%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p>文章内容</p><a id="more"></a><h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>hadoop-worker004、hadoop-worker005有时候会断连接，ambari显示datanode dead，看日志里提示datanode OOM</p><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>查看是由于hadoop-worker004、hadoop-worker005 CPU和IO负载压力太大导致的</p><p>经调查由于sql查询的数据在hadoop-worker004、hadoop-worker005上属于热点数据，且这两个节点数据比较多，需要做rebalance</p><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><h2 id="查看IO负载命令"><a href="#查看IO负载命令" class="headerlink" title="查看IO负载命令"></a>查看IO负载命令</h2><pre><code class="hljs shell">utity占比，越接近100%，磁盘IO压力越大iostat -dmx 1</code></pre>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
